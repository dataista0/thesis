\chapter{Estado de Arte}
\label{chap:estado-de-arte}
% En este capítulo se busca pasar r. 
% En primer lugar presentaremos 
% Discutiremos {\color{red}primero} una serie de investigaciones académicas pequeñas, presentadas en general en congresos y competencias del área, para definir un modelo más o menos estándar del dominio de problemas y los acercamientos típicos. Luego, comentaremos el sistema de IBM -Watson- para ver, más allá de los enfoques usuales, un enfoque exitoso. Finalmente pasaremos revista de una serie de sistemas disponibles para facilitar la creación de modelos de question answering, haciendo foco en Qanus (el framework que utilizamos para una parte nuestra investigación) y mencionando algunos sistemas que evaluamos teóricamente pero que, lamentablemente, no estaban disponibles \textit{out of the box}, principalmente debido a restricciones y modificaciones en el acceso programático que implementaron los buscadores populares en los últimos años\footnote{Ver por ejemplo, Google Search API, deprecado el 1ero de noviembre de 2010 aquí: https://developers.google.com/web-search/}. 

\section{Introducción general}
\label{sec:intro-general-qa}

Como vimos brevemente en la introducción, QA es un área de investigación de ciencias de la computación que busca generar respuesta concretas a preguntas expresadas en algún lenguaje natural. Es, por esto mismo, un problema complejo que involucra herramientas y modelos de otras áreas de existencia autónoma, como information retrieval (IR), procesamiento del lenguaje natural (PLN, NLP) e information extraction (IE). 

También señalamos algunos ejes de subclasificación de problemas de QA. Con respecto al dominio de hechos y conocimientos sobre los que se espera que el sistema sepa responder los sistemas se clasifican como de dominio abierto (\textit{open domain}) o de dominio cerrado (\textit{closed domain}): mientras de los primeros se espera que sepan responder pregunta acerca cualquier tema o dominio, de los segundos solo se espera que sepan responder preguntas acerca de un dominio acotado particular. A su vez, los datos pueden ser estructurados, semi estructurados o no estructurados. El ejemplo típico de una base de conocimientos estructurada es una base de datos relacional, un tipo de datos semi estructurados puede ser un documento xml no normalizado, mientras que el tipo de datos no estructurado por antonomasia es un corpus de documentos en texto plano. 

Las bases de conocimiento de los sistemas de QA pueden tener uno o más tipos de datos, pero cada unos de estos tipos determina un enfoque algorítmico diferente. Por ejemplo, si la base de conocimientos es una DB relacional con un lengueja formal de consultas similar al SQL, el problema de QA típico consiste en encontrar un mappeo desde la pregunta en lenguaje natural a una consulta SQL. Esto implica que el foco de trabajo está en la generación de esta consulta y no en el trabajo posterior sobre los resultados de ejecutarla. Por otro lado, un corpus no estructurado no permite una consulta formal, por lo que el enfoque usual es obtener una lista de documentos relevantes para luego aplicar distintas técnicas de procesamiento de textos para extraer la respuesta buscada.

Cabe agregar que estos dos ejes de clasificación (dominio abierto / dominio cerrado y grado de estructuración de los datos) suelen tener una correlación, a saber: los dominios cerrados suelen disponer (o permitir la construcción sencilla de) una base de conocimientos estructurada, mientras que los dominios abiertos suelen forzar datos no estructurados. Esta correlación no es una asociación. Existen grandes ontologías y diccionarios estructurados que podríamos considerar de dominio abierto, por ejemplo, {\color{red} Yago , dbPedia, Freebase. Mejorar. Linkear}.

Por otro lado, muchos sistemas open domain utilizan métodos de pln sobre corpora no estructurados que luego combinan con verificaciones del tipo de resultado obtenido contra bases de datos estrucutradas. {\color{red} Por ejemplo, IBM-Watson utiliza XXX y Qa-sys utiliza XXX}.

Las siguiente secciones siguen el siguiente formato: presentaremos primero (\ref{sec:intro-general-qa}) una introducción general a la disciplina, repasando su historia (\ref{subsec:historia}), las competencias en torno a las cuales se estructuró en el tiempo y las diferentes métricas utilizadas por estas competencias a la hora de evaluar la performance de los sistemas (\ref{subsec:competencias}), luego (\ref{sec:literatura}) pasaremos revista de diferentes investigaciones sobre question answering para definir un modelo más o menos estándar del dominio de problemas y los acercamientos típicos, discutiendo diferentes investigaciones sobre QA open-domain (\ref{subsec:open-domain}), un enfoque estructurado para dominios cerrados (\ref{subsec:closed-domain}) y, finalmente analizaremos el caso de IBM-Watson (\ref{subsec:ibm-watson}) y otros sistemas (\ref{subsec:otros-sistemas}).


\subsection{Historia de la disciplina}
\label{subsec:historia}
{\color{red}Historia: primeros enfoques, BASEBALL y LUNAR.}

\subsection{Competencias y métricas de evaluación}
\label{subsec:competencias}
{\color{red}Historia de las competencias}
Por ejemplo, la competencia ya finalizada del año 2013, QA4MRE@CLEF2013, (Question Answering for Machine Reading Evaluation) se enfoca principalmente en Machine Reading, tarea que incluye un grado de razonamiento elevado para la computadora\footnote{\url{http://celct.fbk.eu/QA4MRE/}}.

Existen distintas conferencias de evaluación de sistemas QA o de subtareas asociadas (por ejemplo TREC - Text Retrieval Conference \footnote{\url{http://trec.nist.gov/}}-, TAC - Text Analysis Conference \footnote{\url{http://www.nist.gov/tac/}}) - y, a su vez, estas distintas competencias ofrecen distinos llamados a competencias.

La investigación, la producción de software y la formación de una comunidad de i+d vinculada con el question answering estuvo impulsada fuertemente por la competencia TREC. Esta competencia duró de tal año a tal año, momento en el que se suspendió. CLEF. NCTIC. 

\section{Literatura y sistemas}
\label{sec:literatura}
\subsection{Enfoques sobre open domain}
\label{subsec:open-domain}

Existe un consenso general a la ahora de definir el modelo de software más abstracto o de arquitectura para encarar la construcción de un sistema de question answering open domain. Esta arquitectura consiste en un pipeline con al menos tres módulos o compotentes bien diferenciados:
\begin{itemize}
\item Módulo de procesamiento de la pregunta
\item Módulo de procesamiento de documentos
\item Módulo de procesamiento de la respuesta
\end{itemize}

Cada uno de estos módulos cuenta a su vez con subcomponentes sobre los cuales hay mayor o menor consenso, que determinan, en definitiva, la performance del sistema concreto implementado. 

El módulo de procesamiento de la pregunta tiene como subcomponente principal un Clasificador de Preguntas (Ver \allref{subsec:qc} y \allref{sec:stanford-qc}) y puede incorporar otros componente como identificadores del \sq{foco} (\textit{focus}) y del tipo esperado de la respuesta (\textit{answer type}), y también puede incluirse en este módulo un componente encargado de expandir o reformular la pregunta para optimizar los resultados del módulo de procesamiento de documentos ( este subcomponente también se suele considerar como parte del módulo de procesamiento de respuestas). Este último subcomponente tiene diferentes nombres en la literatura (\textit{query reformulation}, \textit{query generation}, \textit{query expansion}, etc).

En el núcleo del módulo de procesamiento de documentos, por lo general, hay uno o más índices invertidos (como Lucene o {\color{red} XXX}) como el descripto en \allref{subsec:indice-invertido}, o bien accesos a un buscador web, o, más en general, estructuras de information retrieval. Como vimos (\ref{subsec:metricas-ir}, existen dos métricas fundamentales a la hora de evaluar el funcionamiento de un módulo de information retrieval: precisión y recall. La precisión es la proporción de documentos relevantes devueltos sobre el total documentos devueltos por el módulo, mientras que el recall es la proporción de documentos relevantes devueltos sobre el total de documentos relevantes existentes en la base de conocimientos. Estas dos métricas suelen comportarse una con la otra como un trade-off, es decir: suelen aparecer escenarios de diseño en los que se deberá optar por priorizar una o bien la otra. Considerando esto, existe una tercera métrica extendida, la medida $F$, que busca parametrizar este compromiso. 

En el contexto de uso de los sistemas de QA, la métrica que se busca maximizar para el módulo de procesamiento de documentos es el recall, es decir, no interesa que el módulo no recupere documentos no relevantes siempre y cuando recupere todos los documentos relevantes existentes que pueda. El motivo es simple: se espera que el análisis lingüístico fino del módulo de procesamiento de la respuesta sea capaz de eliminar texto irrelevante en la misma o en mejor medida que el sistema de information retrieval subyacente al módulo de documentos. Por otro lado, si al recuperar los documentos se filtra una respuesta válida como irrelevante, entonces el resto del pipeline vera sus probabilidades de éxito mermadas (sino directamente anuladas).  La expansión/reformulación de la pregunta del paso anterior apunta, justamente, a generar una query que priorice el recall sobre la precisión. O

Otras tareas que se realizan en el módulo de procesamiento de documentos es la división de los documentos en parágrafos, el filtrado de documentos o parágrafos irrelevantes y el ranking de los resultados. Si bien los sistemas de information retrieval suelen rankear sus resultados, este ranking puede resultar no del todo adecuado para la tarea en cuestión y mejorable teniendo en cuenta la información disponible. 

Finalmente, el módulo de procesamiento de respuestas introduce el valor agregado que distingue un sistema de IR típico y un sistema de QA propiamente dicho. Este módulo es el menos estandarizado de los tres y tiene diferentes enfoques más o menos elaborados en la literatura, pero el grado de consenso sobre la viabilidad del uso de un grupo de técnicas fijas sobre otras no ocurre como, por ejemplo, en el uso de un clasificador de preguntas en el módulo de procesamiento de preguntas o de un índice de búsquedas en el módulo de procesamiento de documentos. Típicamente, el módulo consta de tres momentos: a partir de los parágrafos o documentos rankeados generados por el módulo de documentos se busca identificar oraciones o respuestas candidatas mediante distintas técnicas. Para estas respuestas candidatas identificadas, se extrae la información concreta que responde a la pregunta utilizando heurísticas basadas en las etiquetas obtenidas para la pregunta (clase de pregunta, foco, tipo de respuesta, etc). Finalmente, se implementa algún mecanismo de corroboración o de validación de respuestas, generando una respuesta final.

A continuación nos extenderemos con más detalle sobre los tres módulos propuestos señalando algunos enfoques usuales, usando como guía los papers \cite{QA-survey}, \cite{QA1}, \cite{QA2}, \cite{QA3} y \cite{PASSAGE1}.

\subsubsection*{Módulo de procesamiento de la pregunta}
Este módulo recibe como input una pregunta formulada en algún lenguaje natural y genera como output una representación de la misma que resulte útil para los módulos siguientes. Esto suele realizarse agregando labels o etiquetas (ver Features, en \allref{sec:terminologia}) a la pregunta completa y a sus distintos términos.

La etiqueta principal generada es el tipo de pregunta (\textit{question type}) y para obtenerlo se utiliza un clasificador como el que describimos en (\ref|{subsec:qc})· Lamentablemente, la clasificación de preguntas no está desarrollada para otros idiomas que el inglés, al menos, no con el mismo grado de precisión y visibilidad que, por ejemplo, el clasificador de Stanford \cite{QC1} \cite{QC2}. Como vimos anteriormente, el principal problema a la hora de clasificar una pregunta es la ambigüedad intrinseca de ciertas clases de preguntas. 

A partir del tipo de pregunta se define el tipo de respuesta esperada (\textit{answer type}). No existe un algoritmo automático estándar difundido para generar el tipo de respuesta esperado. El enfoque general encontrado es la aplicación de un mappeo simple basado en reglas, desde el tipo de pregunta generado por el clasificador a una serie de tipos de respuestas predefinido para el dominio de problema.

Otro análisis interesante pero, según nuestra investigación, no sistematizado, es el análisis del \dq{foco} (\textit{question focus}). En \cite{QA3} se define el foco como una palabra o secuencia de palabras que indican qué se está preguntando. Por ejemplo, para la pregunta \dq{¿Quién fue nombrado presidente de Argentina en  1983?} el foco sería \dq{presidente}). {\color{red} VER MI RESUMEN DE QA-SURVER, COMENTARIO PERSONAL}. El concepto aparece también en \cite{WATSON1} pero no encontramos detalles sobre los mecanismos técnicos implicados en la extracción y todo parece señalar a una serie de heurísticas de procesamiento lingüístico escritas a mano. 

Otros análisis que se realizan sobre la pregunta son el POS-tagging (Ver \allref{subsec:pos}), el reconocimiento de entidades nombradas (NER, ver \allref{subsec:ner}) y la búsqueda de dígitos u otros patrones útiles para el contexto. 

Finalmente, se ejecuta la reformulación o expansión de la pregunta para genera un input para el módulo de information retrieval tal que maximice el recall. 
La idea guía es la extracción de keywords relevantes y existen diferentes enfoques, usando los pos-tags y el resultado del ner. También pueden utilizarse recursos léxicos como Wordnet para generar sinónimos de términos importante. 

En \cite{QA1} y \cite{QA3} se propone una heurística basada en 8 reglas para implementar este paso. Las reglas propuestas son:

\begin{itemize}
\item {\color{red} Fill me neddy}
\end{itemize}

\subsubsection*{Módulo de procesamiento de documentos}
\subsubsection*{Módulo de procesamiento de la respuesta}

\subsection{QA como interfaz a una base de datos}
\label{subsec:closed-domain}

\subsection{IBM-Watson}
\label{subsec:ibm-watson}
Watson\cite{WATSON1}\cite{WATSON2} es un sistema diseñado por IBM con el objetivo de competir en
tiempo real en el programa de televisión estadounidense Jeopardy,
logrando resultados del nivel de los campeones humanos de este
programa.

El proyecto demoró 3 años de investigación, en los cuales se
logró obtener la performance esperada (nivel humano experto) en
cuanto a precisión, confiabilidad y velocidad, logrando derrotar a
dos de los hombre con mayores récords históricos del show en un
programa en vivo\footnote{En esta url está disponible el programa en el que el sistema vence a sus competidores humanos: \url{http://www.youtube.com/watch?v=WFR3lOm_xhE}} en febrero de 2011.

El objetivo del \ proyecto puede considerarse una extensión de lo que
fue Deep Blue, el sistema que logró el nivel de los expertos humanos
en el ajedrez, porque buscó superar un reto que significativo y
visible del campo de la Inteligencia Artificial tanto para la comunidad
científica como para la sociedad en general:
{\textquotedblleft}?`puede un sistema computacional ser diseñado para
competir con los mejores hombre en alguna tarea que requiera altos
niveles de inteligencia humana y, si es el caso, que clase de
tecnología, algoritmos e ingenieria se
requiere?{\textquotedblright}\footnote{Traducción propia de
un fragmento de \cite{WATSON1}, p. 2}

Watson es la implementación específica para participar en este
programa de una arquitectura más genérica de question answering,
DeepQA, que da el nombre al proyecto de la corporación. Esta
arquitectura es de construcción reciente y ejemplifica perfectamente la complejidad del problema de
QA de dominio abierto e incorpora tecnologías de punta de distintos
dominios de ciencias de la computación, y de IA en particular:
information retrieval, natural language processing, knowledge
representation and reasoning, machine learning e interfaces humano -
computadora. En el transcurso de esta tesis, IBM lanzó el programa
\sq{Watson Ecosystem} (en noviembre de 2013) que promete la utilización
de tecnología de punta para aplicaciones creadas por la comunidad\footnote{
Announcing the IBM Watson Ecosystem Program: \url{http://www-03.ibm.com/innovation/us/watson/}}.

\subsubsection*{El problema}

Watson debe realizar tareas como parsing, question classification,
question descomposition, automatic source adquisition and evaluation,
entity and relation detection, logical form generation, knowledge
representation and reasoning manteniendo ciertos atributos de calidad
bastante exigentes derivados de la naturaleza del show. Estas
restricciones son:

\begin{itemize}
\item Confiabilidad de la respuesta: \newline
Jeopardy tiene tres participantes con un pulsador y el que desee
responder debe pulsar antes que los demás. Además, existe una
penalización por respuestas incorrectas, por lo que es esencial que
el sistema pueda determinar la confiabilidad de la respuesta obtenida a
fin de optar por responder o no responder.
\item Tiempos de respuesta: \newline
La confiabilidad de la respuesta, o al menos una estimación, debe
calcularse antes de que pase el tiempo para decidir responder (6
segundos) y también de que otro participante oprima su pulsador
(menos de 3 segundos).
\item Precisión:\newline
El tipo de respuestas que se dan en el show suelen ser respuestas
exactas (por ejemplo: solamente un nombre, un número o una fecha,
etc). 
\end{itemize}

\bigskip

El sistema cuenta con varios componentes heurísticos que estiman
ciertos features y grados de confiabilidad para diferentes respuestas,
los cuales son evaluados por un sistema general que sintetiza un grado
de confiabilidad para una respuesta final y determina así si
responder o no responder. 

El programa consta de un tablero con 30 pistas (o preguntas) organizadas
en seis columnas, cada una de las cuales es una categoría. Las
categorías van desde temas acotados como
{\textquotedblleft}historia{\textquotedblright} o
{\textquotedblleft}ciencias{\textquotedblright} hasta temas más
amplios como {\textquotedblleft}cualquier cosa{\textquotedblright} o
{\textquotedblleft}potpourri{\textquotedblright}. Watson intenta
respuestas sobre varias hipótesis de dominio y verifica en cual de
ellos se logran respuestas de mayor confiabilidad. 

Por otra parte, el grueso de las preguntas de Jeopardy son del tipo
\textit{factoid}, esto es, preguntas cuya respuesta esta basada en
información fáctica acerca de una o más entidades individuales.


\bigskip

Por ejemplo:

Categoría: Ciencia General

Pista: Cuando es impactado por electrones, un fósforo emite energía
electromagnética de esta forma

Respuesta: Luz (o fotones)


\bigskip

A su vez, existen ciertos tipos de pistas que requieren un enfoque
particular, por ejemplo, pistos que constan de dos subpistas muy
débilmente relacionadas, o problemas matemáticos formulados en
lenguaje humano, o problemas de fonética, etc, que no pueden ser
simplemente dejados de lado porque, si bien tiene poca probabilidad de
aparición, cuando aparecen lo hacen en bloque y pueden arruinar el
juego de Watson. Se acordó con la productora del programa, sin
embargo, dejar de lado preguntas audiovisuales (aquellas que presentan
una imagen o un audio y requieren interpretarlo) y preguntas que
requieren instrucciones verbales del presentador.


\bigskip

Para determinar el dominio de conocimiento, los investigadores
analizaron 20000 preguntas, extrayendo su LAT (lexical answer type, o
tipo léxico de respuesta). El LAT se define como una palabra en la
pista que indica el tipo de la respuesta esperado. Por ejemplo, para la
pista {\textquotedblleft}Investanda en 1500{\textquoteright}s para
agilizar el juego, este movimiento involucra dos
piezas{\textquotedblright} el LAT es
{\textquotedblleft}movimiento{\textquotedblright}. Menos del 12\% de
las pistas no indicaba explícitamente ningún LAT, usando palabras
como {\textquotedblleft}esto{\textquotedblright} o
{\textquotedblleft}eso{\textquotedblright}. En estos casos, el sistema
debe inferir el tipo de respuesta del contexto. Del análisis de estas
20000 pistas se reconocieron 2500 tipos léxicos distintos, de los
cuales los 200 más frecuentes no llegaban a cubrir el 50\% del total
de pistas. Esto implica que un approach estructurado (orientado por el
tipo de respuesta), si bien resulta útil para algunos tipos, no es
suficiente para abordar el problema completo.

\subsubsection*{Métricas}

Las métricas de resultados, además del tiempo de respuesta, son la
\textit{precisión} (preguntas contestadas correctamente / preguntas
contestadas) y el \textit{porcentaje de respuestas dadas }(preguntas
contestadas / total de preguntas). Mediante la configuración de un
threshold de \textit{confiabilidad} pueden obtenerse distintas
estrategias de juego: un umbral bajo repercutirá en un juego más
agresivo, incrementando la proporción de respuestas contestadas,,
pero disminuyendo su precisión, mientras que un umbral alto
determinará un juego conservador, con menos respuestas dadas pero
mayor precisión en las mismas. Es un clásico escenario de trade-off
entre dos atributos de calidad. Un buen sistema de estimación de
confiabilidad implica una mejora general del sistema, aún cuando el
módulo de generación de respuestas permanezca idéntico.


\bigskip

En el show, el porcentaje de respuestas dadas depende de la velocidad
con la que se llega a presionar el pulsador, lo cual sólo interesa
para el dominio de QA como una restricción temporal. 


\bigskip

Mediante análisis numérico, los investigadores determinaron que los
campeones de Jeopardy lograban tomar entre el 40\% y el 50\% de las
preguntas y, sobre ellas, lograban una precisión de entre el 85\% y
el 95\%, lo que determinaba una barrera de performance bastante
exigente en lo que respecta a QA.


\bigskip

\subsubsection*{Baseline}

El equipo de IBM intentó utilizar dos sistemas consolidados en QA y
adaptarlos al problema \ de Jeopardy. \ El primero fue PIQUANT
(Practical Intelligent Question Answering Technology), un sistema
desarrollado por IBM en conjunto con el programa del gobierno
estadounidense AQUAINT y varias universidades, que estaba entre los
mejores según la TREC (Text Retrieval Conference), una autoridad en
el área. PIQUANT consta de un pipeline típico (véase QANUS) con
tecnología de punta, logrando un rango del 33\% de respuestas
correctas en las evaluaciones TREC-QA. Los requerimientos de la
evaluación de TREC son muy distintos de los de Jeopardy: TREC ofrece
un corpus de conocimiento relativamente pequeño (1M de documentos) de
donde las respuestas deben ser extraídas y justificadas, el tipo de
preguntas de TREC son menos complejas a nivel ling\"uístico que las
de Jeopardy y la estimación de confiabilidad no resulta una métrica
importante (dado que no hay penalización por respuestas incorrectas).
Además, los sistemas tienen permitido acceder a la web y las
restricciones temporales son, por mucho, más amplias (por ejemplo:
una semana para responder 500 preguntas). En Jeopardy, además de las
restricciones ya mencionadas, un requerimiento fue que el sistema
trabaje sobre datos locales y no acceda a la web en tiempo real. El
intento de adaptar PIQUANT al problema de Jeopardy dio pésimos en
comparación con los necesarios: 47\% de precisión sobre el 5\% de
respuestas con mayor confiabilidad y 13\% de precisión en general. 

Por otro lado, el equipo intentó adaptar el sistema OpenEphyra
(véase OpenEphyra), un framework open-source de QA desarrollado en
CMU (Carnegie Mellon University) basado en Ephyra (no libre),
diseñado también para la evaluación TREC. OpenEphyra logra un
45\% de respuestas correctas sobre el set de datos de evaluación TREC
2002, usando busqueda web. La adaptación resultó aún peor que la
de PIQUANT (con menos del 15\% de respuestas correctas y una mala
estimación de la confiabilidad). 

Se probaron dos adaptaciones de estos sistemas. una basada en
búsquedas de texto puro y otra basada en reconocimiento de entidades.
En la primera, la base de conocimiento se modeló de manera no
estructurada y las preguntas se interpretaron como términos de una
query, mientras que en la segunda se modeló una base de conocimientos
estructurada y las preguntas se analizaron semánticamente para
reconocer entidades y relaciones, para luego buscarlos en la base.
Comparando ambos enfoques en base al porcentaje de respuestas dadas, el
primero dio mejores resultados para el 100\% de las respuestas,
mientras que la confiabilidad general era baja; por otro lado, el
segundo enfoque logró altos valores de confiabilidad, pero sólo en
los casos en que efectivamente logra identificar entidades. De aquí
se infiere que cada enfoque tiene sus ventajas, en el dominio de
problemas apropiado.

\subsubsection*{La arquitectura DeepQA}
\label{subsec:deep-qa}
Los intentos de adaptación iniciales, como vimos, no dieron
resultados, así como tampoco sirvieron las adaptaciones de algoritmos
de la literatura científica, los cuales son realmente difíciles de
sacar de su contexto original y de las evaluaciones sobre las cuales
fueron testeados. Este problema, veremos -por ejemplo, con QANUS y
Reverb- , se repitió en nuestro proyecto. Como conclusión de estos
intentos frustrados, el equipo de IBM entendió que una arquitectura
de QA no debía basarse en sus componentes concretos sino en la
facilidad para incorporar nuevos componentes y para adaptarse a nuevos
contextos. Así surgió DeepQA, la arquitectura de base, de la cual
Watson es una instancia concreta para un contexto particular (con
requerimientos de alta precisión, buena estimación de
confiabilidad, lenguaje complejo, amplitud de dominio y restricciones
de velocidad). DeepQA es una arquitectura de computo paralelo,
probabilistico, basado en recopilación de evidencia y scoring. Para
Jeopardy se utilizaron más de 100 técnicas diferentes para analizar
lenguaje natural, identificar y adjudicar valor a fuentes de
información, encontrar y generar hipótesis, encontrar y rankear
evidencias y mergear y rankear hipótesis en función de esta
evidencia. La arquitectura sirvió para ganar Jeopardy, pero también
se adaptó a otros contextos como la evaluación TREC, dando
resultados mucho mejores que sus predecesores. Los principios de
diseño subyacentes de la arquitectura son:

\begin{itemize}
\item Paralelismo masivo\newline
Para evaluar distintas hipótesis en distintos dominios con poco
acoplamiento.
\item Pervasive confidence estimation:\newline
Ningún componente genera la respuesta final, sino que da una serie de
features y grados de confiabilidad y evidencia para distintas
hipótesis, que luego son sintetizados.
\item Integrate shallow and deep knowledge:
\end{itemize}

\bigskip

A continuación, enumeraremos la lista de pasos que sigue el sistema
para obtener la respuesta a una pregunta:

\subsubsection*{Adquisición de contenidos}

El primer paso de DeepQA es la adquisición de contenidos. Este paso es
el único que no se realiza en tiempo de ejecución y consiste en
crear la base de conocimiento en la cual el proceso final buscará la
respuesta a la pregunta, combinando subprocesos manuales y
automáticos. 

En principio se caracteriza el tipo de preguntas a responder y el
dominio de aplicación. El análisis de tipos de preguntas es una
tarea manual, mientras que la determinación del dominio puede
encararse computacionalmente, por ejemplo, con la detección de LATs
que señalamos antes. Dado el amplio dominio de conocimientos que
requiere Jeopardy, Watson cuenta con una gran cantidad de
enciclopedias, diccionarios, tesauros, artículos académicos y de
literatura, etc. A partir de este corpus inicial, el sistema busca en
la web documentos relevantes y los relaciona con los documentos ya
presentes en el corpus. 

Además de este corpus de documentos no estructurados, DeepQA maneja
contenidos semi-estructurados \ y estructurados, incorporando bases de
datos, taxonomías y ontologías como dbPedia, Wordnet y las
ontologías de Yago. 

\subsubsection*{Análisis de la pregunta}

El primer paso en run-time es el análisis de la pregunta. En este paso
el sistema intenta entender qué es lo que la pregunta está
preguntado y realizar los primeros análisis que determinan cómo
encarará el procesamiento el resto del sistema. Watson utiliza
shallow parses, deep parses, formas lógicas, pos-tags,
correferencias, detección de entidades nombradas y de relaciones,
question classification, además de ciertos análisis concretos del
domiento del problema.

En este proceso se clasifica el tipo de la pregunta (los tipos están
determinados por el show: puzzles, matemáticos, etc). También se
busca el tipo de respuesta esperada, dónde los tipos manejados son
por Watson son los LATs extraídos de las preguntas de ejemplo. El LAT
determina el {\textquotedblleft}tipo{\textquotedblright} de la
respuesta, que clase de entidad \textit{es} la respuesta (una fecha, un
hombre, una relación, etc). El equipo de IBM intentó adaptar
distintos algoritmos de clasificación preexistentes, pero después
de intentar entrenarlos para el dominio de tipos de Jeopardy, llegaron
a la conclusión de que su eficacia era dependiente del su sistema de
tipos default, y que la mejor forma de adaptación era mappear su
output a los tipos utilizados por Watson (un enfoque similar fue
utilizado en esta tesis con respecto al clasificador de Stanford). Otra
anotación importante es el
{\textquotedblleft}foco{\textquotedblright} de la pregunta, la parte de
la pregunta tal que si se la reemplaza por la respuesta, la pregunta se
convierte en una afirmación cerrada.

Por ejemplo, para {\textquotedblleft}El hombre que escribió Romeo y
Julieta{\textquotedblright}, el foco es {\textquotedblleft}El hombre
que{\textquotedblright}. Este fragmento suele contener información
importante sobre la respuesta y al reemplazarlo por una respuesta
candidata se obtiene una afirmación fáctica que puede servir para
evaluar distintos candidatos y recolectar evidencia. Por ejemplo,
reemplazando por distintos autores y verificando que la oración
resultante esté presente en el corpus.

Por otro lado, muchas preguntas involucran relaciones entre entidades y,
más puntualmente, tienen una forma sujeto-verbo-objeto. Por ejemplo,
tomando la pista anterior, podemos extraer la relación
\textit{escribir(x, Romeo y Julieta)}. La amplitud del dominio de
Jeopardy hace que la cantidad de entidad y de relaciones entre
entidades sea enorme, pero esto empeora aún más al considerar las
distintas formas de expresar la misma relación. Por eso, Watson
sólo logra encontrar directamente una respuesta mediante
reconocimiento de entidades y relaciones sobre el 2\% de las pistas. En
general, este tipo de enfoque es útil sobre dominios más acotados,
mientras que la detección de relaciones como approach general a un
problema de question answering de dominio amplio es un área de
investigación abierta. 

Una particularidad ya señalada de las preguntas de Jeopardy son las
pistas con subpistas no relacionadas. Para atacar este problema, Watson
genera distintas particiones y resuelve todas en paralelo, sintetizando
las respuesta de cada partición generada mediante algoritmos ad-hoc
de ponderación de confiabilidad y otras características.

\subsubsection*{Generación de hipótesis}

El tercer paso (segundo en run-time) es la generación de hipótesis:
tomando como input el resultado del paso anterior se generan respuestas
candidatas a partir de la base de conocimiento offline. Cada respuesta
candidata reemplazada por el foco de la pregunta es considerada una
hipótesis, que el sistema luego verificará buscando evidencias y
adjudicando un cierto grado de confiabilidad.

En la búsqueda primaria de respuestas candidatas, se busca generar
tantos pasajes como sea posible. El resultado final obtenido revela que
el 85\% de las veces, la respuesta final se encuentra entre los
primeros 250 pasajes devueltos por la búsqueda primaria. La
implementación utiliza una serie variada de técnicas, que incluyen
diferentes motores de búsqueda de textos (como Indri y Lucene),
búsqueda de documentos y de pasajes, búsquedas en bases de
conocimiento estructuradas como SPARQL con triple store y la
generación de mutiples queries a partir de una sola pregunta. La
búsqueda estructurada de triple stores depende del reconocimiento de
entidades y relaciones del paso anterior.

Para un número pequeño de LATs, se definió una suerte de conjunto
de entidades fijas (por ejemplo: países, presidentes, etc). Si la
respuesta final no es retornada en este paso, entonces no hay
posibilidad de obtenerla en los siguiente. Por eso se prioriza el
recall sobre la precisión, con el supuesto de que el resto del
pipeline logrará filtrar la respuesta correcta correctamente. Watson
genera varios cientos de hipótesis candidatas en este paso.


\bigskip

\subsubsection*{(Soft filtering)}

Para optimizar recursos, se realiza un filtrado liviano de respuestas
antes de pasar a la recopilación de evidencia y al scoring de
hipótesis. Un filtrado liviano es, por ejemplo, comprobar similaridad
de la respuesta candidata con el LAT esperado de la respuesta. Aquellas
hipótesis que pasan el filtro pasan al siguiente proceso, que realiza
un análisis más exhaustivo.


\bigskip

\subsubsection*{Recuperación de evidencias y scoring de pasajes}

Para recuperar evidencias se utilizan varios algoritmos. Uno
particularmente útil es buscar la hipótesis candidata junto con las
queries generadas por la pregunta original, lo que señala el uso de
la respuesta en el contexto de la pregunta. \ Las hipótesis con sus
evidencias pasan al siguiente paso, dónde se les adjudica un score. 

El proceso de scoring es donde se realiza la mayor parte del análisis
más fuerte a nivel computacional. DeepQA permite la incorporación
de distintos Scorers, que consideran diferentes dimensiones en las
cuales la hipótesis sirve como respuesta a la pregunta original. Esto
se llevó a cabo definiendo una interfaz común para los scorers.
Watson incorpora más de 50 componentes que producen valores y
diferentes features basados en las evidencias, para los distintos tipos
de datos disponibles (no estructurados, semi estructurados y
estructurados). Los scorers toman en cuenta cuestiones como el grado de
similaridad entre la estrurctura de la respuesta y de la pregunta,
relaciones geoespaciales y temporales, clasificación taxonómica,
roles léxicos y semánticos que se sabe que el candidato puede
cumplir, correlaciones entre terminos con la pregunta, popularidad (u
obscuridad) de la fuente del pasaje, aliases, etc.

POR EJEMPLO: COPIAR NIXON

Los distintos scores se combinan luego en un score único para cada
dimensión.

(Merge)

Recién después de este momento, Watson realiza un merge entre
hipótesis idénticas. Las hipótesis idénticas son diferentes
formulaciones ling\"uisticas de lo mismo, por ejemplo:
{\textquotedblleft}X nació en 1928{\textquotedblright} o
{\textquotedblleft}El año de nacimiento de X es
1928{\textquotedblright}. Finalmente, se procede a estimar un ranking
único y una confiabilidad única para las distintas hipótesis. En
este paso se utilizan técnicas de machine learning que requieren
entrenamiento, y modelos basados en scores. Se utilizan técnicas
jerárquicas como mixture of experts y stacked generalization y,
finalmente, un metalearner fue entrenado para ensamblar los distintos
resultados intermedios. 


\bigskip

\subsubsection*{Tiempos y escala}

DeepQA utiliza Apache UIMA, un framework que implementa UIMA
(Unestructured Information Management Architecture): todos los
componentes de DeepQA son IUMA-annotators, módulos que producen
anotaciones y aserciones sobre un texto.

La implementación inicial de Watson corría sobre un sólo
procesador y demoraba aproximadamente 2 horas en contestar una sola
pregunta. La arquitectura paralela permite, sin embargo, que al
correrlo sobre 2500 núcleos -de la implementación final- los
tiempos de respuesta oscilen entre 3 y 5 segundos, que es lo esperado.

Finalmente, la implementación de Watson logró alcanzar el estándar
de resultados de los campeones de Jeopardy y, como ya dijimos,
compitió y ganó el programa en Febrero de 2011. Además, se
realizaron adaptaciones para trabajar sobre los problemas de TREC, en
los cuales se demostró una amplia mejoría en comparación con
PIQUANT y OpenEphyra


\bigskip

\subsubsection*{Conclusiones sobre IBM-Watson}

System level approach.


\bigskip


\subsection{Otros sistemas de QA: OpenEphyra, Aranea y Just.Ask}
\label{subsec:otros-sistemas}
\bigskip

\subsubsection{No incorporado a QANUS}
En efecto, la arquitectura DeepQA no está disponible para la comunidad, el ya mencionado OpenEphyra, como veremos en breve, no funciona, mientras que otros sistemas resultan igualmente inaccesibles (Aranea, Qanda) mientras que QA-sys es un sistema de QA out of the box. Mencionaremos los logros y los límites de estos objetivos cuando hablemos de nuestro intento por montar nuestro propio sistema sobre Qanus. 

\bigskip


El paper describe las arquitecturas de todos los sistemas, si sirve
meter más info

El paper [EPHYRA1] \ busca crear un criterio cuantitativo para comparar
la eficacia de distintos pasos de Just.Ask, Open Ephyra y Aranea
basándose en la arquitectura \textit{pipeline de tres pasos}
compartida por todos. Los tres sistemas, por lo demás, están
basados en la web, utilizando distintas APIs de buscadores o bien
analizando los resultados de la interfaz de usuario de los mismos. 

El primer ítem importante a destacar de este trabajo, es que, al
momento de la experimentación \textbf{Aranea no funcionaba más y
estaba discontinuado}\footnote{\ (Resaltado en Sección 8, muy
concluyente).\par }\textbf{. }El autor se comunicó con el responsable
del proyecto que corroboró que las APIs de los buscadores en los que
se basaba Aranea cambiaron y no había interés en readaptar el
código para que vuelva a funcionar. Las comparaciones que logró
entre Just.Ask y Open Ephyra son interesantes y concluyentes a favor de
la performance de OpenEphyra. 

(Freeling \ + Cambio de Base + Rigidez)





\chapter{Herramientas y marco de trabajo}

\section{Modelo}

Nuestro sistema resuelve dos problemas de QA similares pero distintos. 

Por un lado, disponemos de una base de datos estructurada, con datos del área de la investigación y la
producción en TICs en Argentina, que requiere un enfoque estructurado con métodos basados en ontologías y en 
formalizaciones de dominio. El enfoque aquí es traducir la pregunta formulada en lenguaje humano a un lenguaje más
formal ``comprensible" según el modelo de dominio. 

Por otro lado, para evaluar métodos de QA 
para datos no estructurados, resolvimos algunos ejercicios de la competencia CLEF del '07. La base de conocimientos para
estos ejercicios son algunos snapshots de Wikipedia en Espa\~nol y en Inglés, anteriores al 2007. Sobre esta base de conocimiento,
el enfoque no es ``traducir" la pregunta a un lenguaje estructurado sino interpretarla y ``compararla", mediante distintas métricas, 
con documentos y pasajes, buscando medidas estádisticas y otras condiciones que permitan \textit{rankear} una respuesta candidata
con un cierto grado de confianza, o bien determinar que no fue posible encontrar una respuesta para la pregunta. 


Conceptualmente, el modelo consiste en los tres pasos típicos: la
creación de la base de conocimientos optimizada, el análisis
lingüístico de la pregunta y la generación de una respuesta desde
la base de conocimientos optimizada a partir de la pregunta con sus
anotaciones. 
Los pasos 1 y 2, la generación de la base de conocimientos optimizada y el procesamiento de la pregunta son procesos
esencialmente análogos para la base de conocimientos estructurada y para la no estructurada. El tercer paso, la generación de respuestas,
tiene distintos enfoques según el caso. 
En las siguientes secciones recorreremos ambos enfoques en conjunto, señalando las decisiones de dise\~no y los distintos módulos
utilizados, haciendo las distinciones pertinentes cuando sea necesario.

En las secciones subsiguiente comentaremos las implementaciones de los
distintos pasos, con sus decisiones de dise\~no y las tecnologías
utilizadas. 


\bigskip



\section{Base de conocimiento}
\bigskip


La creación de la base de conocimiento es el único que se ejecuta offline
y consiste en la obtención del corpus original y en la generación de índices
invertidos. Los índices invertidos, recordamos, son índices que permiten
buscar, para una serie de términos vinculados lógicamente, un conjunto
ponderado de documentos pertinentes. Por su naturaleza, este paso está 
separado de la ejecución del resto del código.


La base de conocimiento de los ejercicios de Clef '07 es mucho más sencilla 
porque no existe ningún modelo \emph{a priori} más allá del documento de lucene.
El formato de la entidad ``artículo", como señalamos antes, es: $(id, titulo, cuerpo)$. 
En este caso, el trabajo más fino no está en el modelado inicial del dominio 
sino la capacidad lingüística de extraer pasajes a partir de un artículo y recomponer información
estructurada a partir de estos pasajes. Mientras que para un modelo estructurado la base de conocimiento
debería permitirnos, para un cierto input, identificar univocamente una entidad y darnos pistas sobre un pedido de
información acerca de esa entidad, el objetivo sobre un corpus de documentos en 
traer todos los documentos en los que sea posible que exista un pasaje respondiendo a la pregunta o
evidencia relevante para apoyar una respuesta. Es decir, mientras una respuesta acotada es una virtud para
el manejador de una base de conocimientos estructurada, el manejador de una lista de documentos de texto debería devolver
una lista lo suficientemente grande para contener la respuesta dentro de los pasajes. La razón de esta política es que si
por ser demasiado estrictos a la hora de retornar documentos llegasemos a descartar un pasaje candidato válido esto
redundaría en una baja generar de efectividad, mientras que en pasos subsiguiente será trivial descartar toda información irrelevante
sin tanto costo. Por eso, el acceso a los indices de wikpedia consta simplemente de un generador de queries similar al recién comentado
accediendo y acumulando resultados (rankeados) a partir de un $LuceneIndexReader$ común (Ver \ref{sec:lucene} \nameref{sec:lucene} para más información).


\bigskip

\section{Análisis de la pregunta}
\label{sec:qprocess}
En este paso se realizan diferentes análisis lingüísticos de la pregunta.
El resultado son distintas características asociadas a la pregunta (anotaciones)
y distintas entidades semánticas reconocidas útiles para el proceso de generación de respuestas. 
Las herramientas de procesamiento de lenguaje natural que utilizamos en 
la implementación de este paso del pipeline 
incluyen: detección de lenguaje, extracción y verificación de entidades nombradas (NER), 
de verbos, sustantivos, qwords (qué, quién, cómo) (POS), análisis de n-gramas y categorización por tipo de pregunta (QC).

El proceso de análisis de la pregunta es bastante similar para ambos approachs (estructurado y no estructurado), por lo
que comentaremos ambos en simultaneo, mencionando diferencias cuando corresponda. 
Estructuraremos esta parte de la tesis en las siguientes secciones:

\begin{itemize}
\item Detección de idioma 
\item Detección y verificación de entidades nombradas
\item Análisis gramatical
\item Clasificación del tipo de pregunta
\end{itemize}

Si bien es cierto que el segundo item está basado principalmente en NER-tagging, el tercero en POS-tagging y el cuarto en Question Clasiffication, 
cada uno de estos pasos utiliza estas herramientas de diferentes maneras. Por ejemplo, para la detección y verificación de entidades del analisis estructurado, además del NER-tagger también utilizamos la base de conocimiento y el análisis gramatical y, para el español, la clasificación del tipo de pregunta se hace apoyandose en las qwords identificadas por el POS-tagger.





\subsection{Entidades nombradas}
\label{subsec:impl-ner}

Para la detección de entidades utilizamos la clase simple de detección (NER) y clasificación (NEC) de entidades de Freeling y el NERC de Stanford (ver \allref{sec:freeling} y \allref{sec:stanford-ner}). Las herramientas de Stanford en general superan a las de Freeling -al igual que el detector de idiomas de Cybozu-, pero solo sirven para inglés. La clasificación utilizada por ambos es la más general de las comentadas en \allref{subsec:nerc}: persona, lugar, organización y otros. 

Veamos algunos ejemplos de funcionamiento de los módulos de detección de entidades. 

\begin{center}
\begin{tabular}{| p {8cm} | l | l | l |}
\hline
Texto & Freeling & Stanford & Resultado \\ \hline
¿Dónde queda la Universidad de Buenos Aires? & es & es & es \\ \hline
Where is located the University of Buenos Aires? & en & en & en \\ \hline
Where is located the Univesidad de Buenos Aires? & en & en & en \\ \hline
Where is located Universidad de Buenos Aires? &  {\color{red}es} & en & en \\ \hline
Quién es Carolina Fernandez? & es & es & es \\ \hline
Who is Carolina Fernandez? &  {\color{red}none} & en & en \\ \hline
Quién es John McCain? & {\color{red}none} & es & es \\ \hline
Who is John McCain? & en & en & en \\ \hline
Dónde vive John McCain y por qué vive allí? & es & es & es \\ \hline
Where does Carolina Fernandez live and why does she lives there? & en & en & en \\ \hline
\end{tabular}
\end{center}

\medskip

Mientras la detección de entidades para los ejercicios de Clef se detiene en el reconocimiento de entidades nombradas a nivel lingüísitico, para el sistema estructurado el proceso es un poco más complejo. Esto se debe a que en este caso la detección de entidades es esencial. Si en el proceso de anotado de la pregunta no se logra identificar alguna entidad reconocida por el modelo de datos, entonces se está muy lejos de encontrar una respuesta. Por eso, además de utilizar los modulos NER recién mencionado, agregamos otros algoritmos de detección y, también, verificación de entidades. 

En principio, verificamos la o las entidades nombradas reconocidas contra la base de conocimiento. El $KnowledgeManager$ ofrece diferentes servicios de verificación de entidades. Para una cadena de tokens cualquiera, este módulo puede decidir, con un cierto grado de confianza, si:

\begin{itemize}
  \item La cadena de tokens es una entidad dentro del modelo de datos. Esto incluye:
    \begin{itemize}
      \item Es una entidad del modelo de datos: una universidad, una empresa, un investigador, un proyecto, una publicacion o una tematica
      \item Es una entidad inferida: una ciudad, una provincia, un centro de investigación, un lugar de trabajo
    \end{itemize}
  \item La cadena es una colección del modelo de datos, es decir, si se están nombrando \dblquote{Investigadores} o \dblquote{Universidades} como clase de entidades.
  \item La cadena es un atributo o una relación de una clase. (nombre de investigador)
\end{itemize}

Parte importante del trabajo para este esquema es lograr identificar este tipo de entidades lingüísticas, por lo que además de verificar los resultados del proceso de NER-tagging, también generamos otras cadenas de input. Notar además que los nombres de clase y de atributos de clase no tendrían por que ser reconocidas por el NER-tagger. Por ejemplo, para ``¿Qué investigadores trabajan en Córdoba?", \dblquote{investigadores} está haciendo referencia al nombre de una clase pero no es el tipo de entidades lingüísticas que detecta un NER-tagger. 

Por estas razones, generamos más entidades lingüísticas posibles además de las entidades detectadas por los NER-taggers. Una vez que todas las entidades nombradas fueron verificadas, generamos n-gramas sobre el resto de la pregunta para chequear por más tokens reconocibles. Configuramos la generación de n-gramas de 1 a 3, con ciertos filtros para no verificar construcciones que no representan entidades de manera trivial. Por ejemplo: dejamos sólo los unigramas que cumplan el rol de sustantivos, eliminamos bigramas que sean un sustantivo y una acción, salteamos NERs ya reconocidas, etre otros.


\subsection{Análisis gramatical}
\label{subsec:impl-pos}
De las diferentes etiquetas que generan los POS-taggers, en nuestro sistema distinguimos los verbos, los sustantivos, las qwords y las palabras triviales. 
Las palabras etiquetadas cumplen distintos roles a lo largo del proceso de generación de respuestas. Como señalamos recién, los n-gramas que se verifican contra la base de datos están filtrados por los roles gramaticales de sus tokens. Por otro lado, a la hora de generar el tipo de pregunta para una pregunta en español, utilizamos, como mecanismo ad-hoc, las qwords. 

\begin{center}
\begin{tabular}{| l | l |}
\hline
Clase & Ejemplos\\ \hline
qword  & qué, quién, cómo, dónde, cuándo\\ \hline
verbo & trabaja, trabajar, trabajando \\ \hline
trivial  & lo, a, de, y \\ \hline
sustantivos  & universidad, impresora, álgebra \\ \hline
\end{tabular}
\end{center}

Los usos más intensivos de estas etiquetas son el filtrado de n-gramas que describimos en la sección anterior para el caso estructurado (\allref{subsec:impl-ner}), un algoritmo ad-hoc de etiquetado de Q-Type para el caso español (es el próximo tema a discutir en \allref{subsec:qtype}) y, para la generación de respuestas, las ponderaciones de pasajes en los scorers de los ejercicios de Clef (\allref{subsec:scorers}), y finalmente, sirven para desempatar por atributos o relaciones preguntadas en algunos casos del modelo estructurado.

\subsection{Clasificación}
\label{subsec:qtype}
Para clasificar la pregunta según su tipo de respuesta esperada utilizamos el Question Classiffier de Stanford, tomando la configuración de Qanus. Este clasificador arroja una clase y una subclase (ver \allref{sec:stanford-qc} para una lista detallada de las posibles clases) y un grado de confianza para esta asignación. A continuación presentamos algunos ejemplos que ilustran estos resultados.

\begin{center}
\begin{tabular}{| l | l | l |}
\hline
Pregunta & Clase y Subclase & Confianza\\ \hline 
What's his name? & HUM:ind & 0.74 \\ \hline 
Where do you come from? & DESC:desc & 0.62 \\ \hline 
What's your phone number? & NUM:code & 0.63 \\ \hline 
How old are you? & NUM:period & 0.78 \\ \hline 
When were you born? & NUM:date & 0.99 \\ \hline 
What does he look like? & DESC:desc & 0.82 \\ \hline 
\end{tabular}
\end{center}

Sin embargo, como ya señalamos oportunamente (ver \allref{subsec:qc}), no existen herramientas de clasificación de preguntas para el idioma español. Esto nos llevó a tomar diferentes medidas para aproximar un tipo de pregunta y no tener distintos casos de código para el inglés y el español. Para las preguntas de Clef formuladas en español, utilizamos su versión en inglés para obtener el tipo.
Para este caso, el tipo de respuesta esperada de \dblquote{¿En qué colegio estudia Harry Potter?} es el mismo que el tipo de respuesta esperada de ``In what school does Harry Potter study?" (ENTY:cremat con 0.22 de confianza). Para el caso estructurado, al no disponer de una traducción segura, escribimos un clasificador básico basado en reglas escritas a mano sobre QWords. Las qwords son palabras clave de las preguntas que señalan el tipo de respuesta. Por ejemplo: una pregunta que comienza con `Cuándo' tendrá como tipo de respuesta una fecha, un tiempo, etc. En el modelo estructurado, definimos una serie acotada de categorías de tipo de respuesta esperadas y unificamos los resultados del clasificador de Stanford y nuestras reglas sobre qwords para español para unificar el código. Estas categorías son:  Who, Whom, Where, Which,  When,  What y Other. Es decir: Quién, Quiénes, Dónde, Cuál, Cuándo, Qué y otros. Las clases y subclases del clasificador de Stanford se mapearon a estas categorías que coinciden con los resultados de las reglas escritas a mano para el español. 



\section{Generación de Respuestas}

El proceso de generación de respuestas difiere sustancialmente entre ambos modelos de dominio. Para el sistema basado en datos estructurados, el approach es determinan en casos de código las distintas posibilidades, acotadas, de las cosas que se pueden responder. Nuestro dominio es tal que solo se pueden responder entidades, atributos de entidades o relaciones entre entidades. De ese modo, si no es posible redirigir el flujo de la pregunta hacia alguna respuesta conocida, no hay posibilidad de articular una respuesta significativa. Para el caso de los ejericicios de Clef sobre wikipedia, el enfoque es muy distinto. En primer lugar, no hay un modelo de los datos del dominio, hay textos con pasajes (u oraciones). Si bien es posible una cierta jerarquización de los datos (por ejemplo, utilizando los nombres de los articulos como verificación de la existencia de una entidad), un enfoque estructurado resulta imposible. En este contexto se utiliza la técnica de rankeo semántico de pasajes en base a features (características). Estas dimensiones de valoración de los pasajes son llamados Scorers (ver \allref{subsec:scorers}). Los Scorers, como veremos, pueden ser tan sencillos como preferir minimamente una cierta longitud sobre otra y también pueden incorporar dimensiones de análisis lingüístico (por ejemplo, la presencia de cierta entidad en un cierto rol semántico). El algoritmo de generación de respuestas consiste, en el caso no estructurado, en encontrar features útiles, significativos y en establecer mecanismo inteligentes de priorización de estos features. 
