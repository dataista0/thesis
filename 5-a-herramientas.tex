\appendix
\chapter{Herramientas}

\section{Stanford Question Classifier}
\label{sec:stanford-qc}
The question classifier (QuestionClassifier) is implemented with the 
STANFORD CLASSIFIER (Manning and Klein 2003), using the question taxonomy\footnote{http://cogcomp.cs.illinois.edu/Data/QA/QC/definition.html} 
explained in (Li and Roth 2002) and is responsible for finding out the expected 
answer type of a given question.

Li, Xin, and Dan Roth. "Learning Question Classifiers." International Conference 
on Computational Linguistics (COLING).2002.
Manning, Christopher, and Dan Klein. "Optimization, Maxent Models, and 
Conditional Estimation without Magic." Tutorial at HLT-NAACL and ACL. 2003.

This software is a Java implementation of a maximum entropy classifier.
Maximum entropy models are otherwise known as conditional loglinear
models, and are essentially equivalent to multiclass logistic
regression models (though parameterized slightly differently, in a way
that is advantageous with sparse explanatory feature vectors). 

\begin{center}
\begin{tabular}{| l | l |}
\hline
Clase y subclase & Definición \\ \hline
ABBREVIATION &  abbreviation \\ \hline 
  abb & abbreviation\\ \hline 
  exp & expression abbreviated\\ \hline 
ENTITY  & entities\\ \hline 
  animal  & animals\\ \hline 
  body & organs of body\\ \hline 
  color & colors\\ \hline 
  creative & inventions, books and other creative pieces\\ \hline 
  currency & currency names\\ \hline 
  dis.med. & diseases and medicine\\ \hline 
  event & events\\ \hline 
  food & food\\ \hline 
  instrument & musical instrument\\ \hline 
  lang & languages\\ \hline 
  letter & letters like a-z\\ \hline 
  other & other entities\\ \hline 
  plant & plants\\ \hline 
  product & products\\ \hline 
  religion  & religions\\ \hline 
  sport & sports\\ \hline 
  substance & elements and substances\\ \hline 
  symbol & symbols and signs\\ \hline 
  technique & techniques and methods\\ \hline 
  term  & equivalent terms\\ \hline 
  vehicle & vehicles\\ \hline 
  word & words with a special property\\ \hline 
DESCRIPTION & description and abstract concepts\\ \hline 
  definition & definition of sth.\\ \hline 
  description & description of sth.\\ \hline 
  manner & manner of an action\\ \hline 
  reason & reasons\\ \hline 
\end{tabular}
\begin{tabular}{| l | l |}
\hline
HUMAN & human beings\\ \hline 
  group & a group or organization of persons\\ \hline 
  ind & an individual\\ \hline 
  title & title of a person\\ \hline 
  description & description of a person\\ \hline 
LOCATION & locations\\ \hline 
  city & cities\\ \hline 
  country & countries\\ \hline 
  mountain & mountains\\ \hline 
  other & other locations\\ \hline 
  state & states\\ \hline 
NUMERIC & numeric values\\ \hline 
  code  & postcodes or other codes\\ \hline 
  count & number of sth.\\ \hline 
  date  & dates\\ \hline 
  distance &  linear measures\\ \hline 
  money & prices\\ \hline 
  order & ranks\\ \hline 
  other & other numbers\\ \hline 
  period  & the lasting time of sth.\\ \hline 
  percent & fractions\\ \hline 
  speed & speed\\ \hline 
  temp & temperature\\ \hline 
  size & size, area and volume\\ \hline 
  weight & weight\\ \hline 
\end{tabular}
\end{center}


\section{Stanford POS Tagger}

Part of Speech Tagging Stanford POS Tagger (Toutanova and Manning 
2000)

Toutanova, Kristina, and Christopher Manning. "Enriching the Knowledge 
Sources Used in a Maximum Entropy Part-of-Speech Tagger." Proceedings of 
the Joint SIGDAT Conference on Empirical Methods in Natural Language 
Processing and Very Large Corpora. 2000. 63-70.


\section{Stanford NER Tagger}
\label{sec:stanford-ner}

Named Entity Recognition Stanford Named Entity Recognizer (Finkel, 
Grenager and Manning 2005) 

Finkel, Jenny Rose, Trond Grenager, and Christopher Manning. "Incorporating 
Non-local Information into Information Extraction Systems by Gibbs Sampling." 
roceedings of the 43nd Annual Meeting of the Association for Computational 
Linguistics. 2005. \cite{NER2}


\section{Freeling}
\label{sec:freeling}
Freeling es una librería de c\'odigo abierto que provee distintas herramientas de 
procesamiento de lenguaje natural, desarrollada y mantenida por el Centre de Tecnologies 
i Aplicacions del Llenguatge i la Parla (TALP) de la Universidad Politécnica de Catalu\~na (UPC). 
Freeling está dise\~nado para ser usada como una librería externa y ofrece un API en distintos lenguajes
de programaci\'on. Su principal virtud es ser multilingüe, esto es, los diferentes analizadores que provee funcionan 
para un conjunto bastante amplio de idiomas. La última versi\'on a la fecha (3.1) soporta los siguientes idiomas:

\begin{itemize}
\item Asturian (as)
\item Catalan (ca) 
\item English (en)
\item French (fr) 
\item Galician (gl)
\item Italian (it)
\item Portuguese (pt)
\item Russian (ru)
\item Slovene (sl)
\item Spanish (es)
\item Welsh (cy)
\end{itemize}

Cabe destacar que no todos los m\'odulos soportan todos los idiomas. Sin embargo, dado que el proyecto está radicado en Espa\~na,
los idiomas necesarios para los fines de nuestro trabajo (espa\~nol e inglés), soportan todos los m\'odulos disponibles
en la librería.
Freeling 3.1 ofrece los siguientes analizadores lingüisticos:

\begin{itemize}
\item Detecci\'on de idioma
\item Tokenizer
\item Sentence splitting,
\item Análisis morfol\'ogico
\item NER y NEC (Detecci\'on y Clasificaci\'on de Entidades Nombradas)
\item Reconocimiento de fechas, números, magnitudes físicas, monedas
\item Codificaci\'on fonética
\item POS tagging, 
\item Shallow parsing
\item Dependency parsing
\item Wordnet-based sense annotation
\item Word Sense Disambiguation
\item Coreference resolution
\end{itemize}



\section{Lang Detect de Cybozu Labs}
\label{sec:cybozu}

Librería de Cybozu Labs - una compañía japonesa -, implementado en Java y liberado bajo Apache License 2.0. En la práctica, este paquete dio excelentes resultados. Soporta 53 idiomas con \%99 de precisión para todos ellos (según sus tests). El detector se basa en perfiles de idiomas generados a partir de las distintas wikipedias y detecta el idioma de los textos usando un filtro bayesiano ingenuo (\textit{naive bayesian}).
El código está disponible, actualmente, en google-code (El link está en la sección de bibliografía \cite{nakatani2010langdetect})

\section{MyMemory}
MyMemory es la Memoria de Traducción más grande del mundo: 300 millones de segmentos a finales de 2009

Como las MT tradicionales, MyMemory almacena segmentos con sus traducciones, ofreciendo a los traductores correspondencias y concordancias. El proyecto se diferencia de las tecnologías tradicionales por sus ambiciosas dimensiones y por su arquitectura centralizada y basada en la colaboración colectiva. Todos pueden consultar MyMemory o hacer aportaciones a través de Internet; la calidad de las aportaciones será cuidadosamente ponderada.
MyMemory gives you quick access to a large number of translations originating from professional translators, LSPs, customers and multilingual web content. It uses a powerful matching algorithm to provide the best translations available for your source text. MyMemory currently contains 644.377.834 professionally translated segments.

Las memorias de traducción son almacenes compuestos de textos originales en una lengua alineados con su traducción en otras. Esta definición de memorias de traducción coincide literalmente con una de las definiciones más aceptadas de corpus lingüístico de tipo paralelo (Baker, 1995). Por esto se puede decir que las memorias de traducción son corpus paralelos.


Por el momento, no estamos utilizando ningún módulo de traducciones:
todo el enfoque multilingüe está dado por la detección del idioma
de la pregunta y la determinación de distintas herramientas de
análisis según qué idioma sea. Sin embargo, en un momento se
evaluó un enfoque distinto, basado en la traducción. Por ejemplo:
utilizar módulos de procesamiento sólo en inglés y
{\textquotedblleft}normalizar{\textquotedblright} los inputs en otros
idiomas (en principio, en espa\~nol), a este idioma interno, y luego lo
mismo con la generación de respuestas. A pesar de que no es el
enfoque actual, hubo una fase de investigación dentro del dominio de
la traducción, que resultó en un módulo de traducción basado en
MyMemoryAPI.

Intento con google translator y la privatización. ?`La falta de
software de traducción offline? El módulo de mymemory, robado de
algún lugar. El sistema de cobro. 

\section{Reverb}

ReVerb is a program that automatically identifies and extracts binary relationships from English sentences. ReVerb is designed for Web-scale information extraction, where the target relations cannot be specified in advance and speed is important.

ReVerb takes raw text as input, and outputs (argument1, relation phrase, argument2) triples. For example, given the sentence "Bananas are an excellent source of potassium," ReVerb will extract the triple (bananas, be source of, potassium).

\section{Apache Lucene}
\label{sec:lucene}
Lucene es una librería de information retrieval, de c\'odigo abierto, escrita en Java y distribuida 
bajo la licencia Apache Software License por la Apache Software Foundation. No está pensada para
usuarios finales sino para ser integrada dentro de proyectos informáticos, resolviendo
la parte de bajo nivel y brindando servicios a través de un API en diferentes lenguajes de programaci\'on.
Su core es un índice invertido como el que describimos anteriormente. La implementaci\'on de un sistema
que utiliza Lucene consta de dos pasos separados:
\begin{itemize}
\item La \textbf{creaci\'on} del índice, es por lo general un proceso offline en el cual 
se incorporan distintas fuentes de informaci\'on al índice 
\item La \textbf{búsqueda} de documentos en el índice creado en el paso anterior, a partir de una query 
ingresada por el usuario final. Este proceso se incorpora dentro del flujo `online' del sistema.
El resultado de esta búsqueda es una lista de documentos rankeados con un cierto puntaje. 
\end{itemize}

Es importante señalar que si bien el proceso de creaci\'on del índice suele estar desacoplado del resto 
del sistema, las fuentes de informaci\'on no tiene por que ser `offline' en el sentido de ser documentos
en un disco local. De hecho, Nutch, otro proyecto de c\'odigo abierto de la Apache Software Foundation es 
un motor de búsqueda web basado en Lucene que incorpora un crawler para indexar sitios web. Lucene soporta 
cualquier fuente de informaci\'on que pueda convertirse en texto mediante algoritmia.
\newline
Los conceptos fundamentales de Lucene son: índice, documento, campo, término y query.
\begin{itemize}
\item Un índice contiene un conjunto de documentos
\item Un documento es un conjunto de campos
\item Un campo es el nombre de una secuencia de términos
\item Un término es un token (una palabra)
\item Una query es una lista de términos conectados con distintos operados l\'ogicos
\end{itemize}

\bigskip
[[Dar ejemplos de una query]]
\bigskip

\section{gwtwiki -Java Wikipedia API}\label{sec:gwtwiki}

Java Wikipedia API (Bliki engine)
http://code.google.com/p/gwtwiki/
Esta librería tiene métodos útiles para trabajar con dumps de wikipedia. La usamos para testear los métodos no estructurados de la tesis.


\section{Modelos de Morphia}\label{sec:modelos-morphia}

Java Wikipedia API (Bliki engine)
http://code.google.com/p/gwtwiki/
Esta librería tiene métodos útiles para trabajar con dumps de wikipedia. La usamos para testear los métodos no estructurados de la tesis.


\chapter{Comparadores}
\label{sec:comparadores}

Dada la frecuencia en la que resultaba necesario comparar dos string,
decidimos reificar la operación de comparación como una familia de
clases que implementan Comparadores. 


Gracias a esto se hace posible cambiar las nociones de igualdad o
similaridad en un módulo completo del sistema o en alguna clase
simplemente configurando otro comparador como parámetro. La
interfaz permite a los distintos comparadores tomar valores binarios
así como también valores entre 0 y 1. A su vez, esta considerada la
posibilidad de configurar un umbral (threshold) a partir del cual
redondear un valor entre 0 y 1 a un valor binario. Otro factor que tuvo
mucha utilidad fue la capacidad de anidar comparadores. 
El concepto de comparador incluye cualquier operación que tome dos
strings y genere un resultado booleano o analógico. Es decir, es posible 
incorporar análisis lingüísticos o queries a la base de datos en ellos.
También se incorpora la posibilidad de ignorar o no ignorar la diferencia de mayúsculas, 
y obviar o no las tildes y otros signos problemáticos. Los comparadores sirven,
en general, para comparar tanto strings representando palabras como
string representado listas de palabras (oraciones o textos). Algunos,
en particular, sólo sirven para este segundo caso. Los comparadores
que finalmente utilizamos en esta tesis son los siguientes.

\begin{center}
\begin{tabular}{| l | p {8cm} |}
\hline
\multicolumn{2}{|c|}{Comparadores de Strings} \\ \hline
Nombre & Descripción\\ \hline 
Equal & Compara por igualdad estricta \\ \hline 
EqualNoPunct &  Compara por igualdad, eliminando signos de
puntuación y normalizando acentos y otras posibles diferencias que no
deberían tenerse en cuenta. \\ \hline 
Contains & Verifica si un string contiene a otro. Puede usar Equal o EqualNoPunct \\ \hline 
EditDistance & Verifica cuan similares son dos string contando la mínima cantidad de operaciones requeridas para transformar un string en el otro \\ \hline 
\end{tabular}
\end{center}

Los siguiente comparadores son algoritmos fueron adaptados a partir de
los Scorers del proyecto Qanus. Todos devuelven valores reales entre
0 y 1 y sirven para comparar secuencias de tokens (y no sólo palabras). Estos
comparadores, al igual que Contains, no son simétricos. Para
distinguir, llamaremos primer string al buscado y segundo string a
aquel en el cual se busca el primero. 

\begin{center}
\begin{tabular}{| l | l | p {8cm} |}
\hline
\multicolumn{3}{|c|}{Comparadores de Secuencias de Tokens} \\ \hline
Abreviatura & Nombre &  Descripción\\ \hline 
Freq & Frequencia & Computa la cantidad de veces que los tokens del primer
string ocurren en el segundo string. Esta suma se divide por la
longitud del segundo string, dando un valor entre 0 y 1. \\ \hline 
Covr & Cobertura &  Computa cuantos tokens del primer string aparecen al
menos una vez en el segundo, y divide esta suma por el total de tokens
del \textit{primer} string.\\ \hline 
Prox & Proximidad &  Computa la distancia entre dos strings en un tercero. Ver abajo.   \\ \hline 
Span & Distancia entre tokens & Computa la distancia media entre términos del primer string en el segundo. Ver abajo. \\ \hline
\end{tabular}
\end{center}

Vamos a explicar los algoritmos de $Prox$ y $Span$ ya que no son triviales. \newline
$Prox$ toma dos strings a buscar en un tercero. Busca ambos en el tercero y computa la distancia en tokens entre ellos.
Esta distancia se calcula como la distancia entre el centro de ambos strings.
Por ejemplo, para los strings de búsqueda \dq{Argentina es un país americano} y \dq{independizado en 1810} sobre el texto \dq{Argentina es un país americano, originalmente una colonia española, independizado en 1810} se considera la distancia entre \sq{un} y \sq{en} (por ser los tokens \sq{intermedios} de ambos strings
de búsqueda. La distancia entre ambos, en el tercer string, es 7. Esta distancia se divide por la longitud en tokens del string en el que se buscan (12), dando un resultado de 0.58. Un score cercano a 1 denota que los dos string están cercanos uno al otro en el tercer string. \newline
Por su parte, $Span$ tiene un concepto similar, pero funciona sobre un solo string de búsqueda, considerando sus tokens. Los distintos tokens buscados ocurren en ciertas posiciones. $Span$ considera la distancia entre las posiciones de los tokens más distantes, dividiendo el total de tokens encontrados por este valor.
Un score cercano a 1 significa que los términos del string buscado están cerca en el string en el que se buscan.
Por ejemplo, suponiendo los siguientes matchs de tokens (denotados por una X): \newline
..... X ..... X ..... X ...... \newline
......a ...... b ...... c ...... \newline

El score de $Span$ estaría dado por \#total de tokens encontrados /  {\textbar}c-a{\textbar}.
