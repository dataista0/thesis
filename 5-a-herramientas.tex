\appendix
\chapter{Herramientas}

\section{Stanford Question Classifier}
The question classifier (QuestionClassifier) is implemented with the 
STANFORD CLASSIFIER (Manning and Klein 2003), using the question taxonomy 
explained in (Li and Roth 2002) and is responsible for finding out the expected 
answer type of a given question

Li, Xin, and Dan Roth. "Learning Question Classifiers." International Conference 
on Computational Linguistics (COLING).2002.
Manning, Christopher, and Dan Klein. "Optimization, Maxent Models, and 
Conditional Estimation without Magic." Tutorial at HLT-NAACL and ACL. 2003.

This software is a Java implementation of a maximum entropy classifier.
Maximum entropy models are otherwise known as conditional loglinear
models, and are essentially equivalent to multiclass logistic
regression models (though parameterized slightly differently, in a way
that is advantageous with sparse explanatory feature vectors). 

\section{Stanford POS Tagger}

Part of Speech Tagging Stanford POS Tagger (Toutanova and Manning 
2000)

Toutanova, Kristina, and Christopher Manning. "Enriching the Knowledge 
Sources Used in a Maximum Entropy Part-of-Speech Tagger." Proceedings of 
the Joint SIGDAT Conference on Empirical Methods in Natural Language 
Processing and Very Large Corpora. 2000. 63-70.


\section{Stanford NER Tagger}

Named Entity Recognition Stanford Named Entity Recognizer (Finkel, 
Grenager and Manning 2005)

Finkel, Jenny Rose, Trond Grenager, and Christopher Manning. "Incorporating 
Non-local Information into Information Extraction Systems by Gibbs Sampling." 
roceedings of the 43nd Annual Meeting of the Association for Computational 
Linguistics. 2005.


\section{Freeling}

Freeling es una librer\'ia de c\'odigo abierto que provee distintas herramientas de 
procesamiento de lenguaje natural, desarrollada y mantenida por el Centre de Tecnologies 
i Aplicacions del Llenguatge i la Parla (TALP) de la Universidad Polit\'ecnica de Catalu\~na (UPC). 
Freeling est\'a dise\~nado para ser usada como una librer\'ia externa y ofrece un API en distintos lenguajes
de programaci\'on. Su principal virtud es ser multilingüe, esto es, los diferentes analizadores que provee funcionan 
para un conjunto bastante amplio de idiomas. La \'ultima versi\'on a la fecha (3.1) soporta los siguientes idiomas:

\begin{itemize}
\item Asturian (as)
\item Catalan (ca) 
\item English (en)
\item French (fr) 
\item Galician (gl)
\item Italian (it)
\item Portuguese (pt)
\item Russian (ru)
\item Slovene (sl)
\item Spanish (es)
\item Welsh (cy)
\end{itemize}

Cabe destacar que no todos los m\'odulos soportan todos los idiomas. Sin embargo, dado que el proyecto est\'a radicado en Espa\~na,
los idiomas necesarios para los fines de nuestro trabajo (espa\~nol e ingl\'es), soportan todos los m\'odulos disponibles
en la librer\'ia.
Freeling 3.1 ofrece los siguientes analizadores lingüisticos:

\begin{itemize}
\item Detecci\'on de idioma
\item Tokenizer
\item Sentence splitting,
\item An\'alisis morfol\'ogico
\item NER y NEC (Detecci\'on y Clasificaci\'on de Entidades Nombradas)
\item Reconocimiento de fechas, n\'umeros, magnitudes f\'isicas, monedas
\item Codificaci\'on fon\'etica
\item POS tagging, 
\item Shallow parsing
\item Dependency parsing
\item Wordnet-based sense annotation
\item Word Sense Disambiguation
\item Coreference resolution
\end{itemize}



\section{Lang Detect de Cybozu Labs}

Para la detecci\'on de idiomas utilizamos tanto el m\'odulo que brinda Freeling como una librer\'ia de Cybozu Labs - una reconocida compañ\'ia japonesa,
implementado en Java y liberado bajo Apache License 2.0. En la pr\'actica, este paquete dio excelentes resultados. Soporta 53 idiomas con \%99 de precisi\'on
para todos ellos (seg\'un sus tests). El detector se basa en perfiles de idiomas generados a partir de las distintas wikipedias y detecta el idioma de los texots
usando un filtro bayesiano ingenuo (\textit{naive bayesian}).

\section{MyMemory}
MyMemory es la Memoria de Traducción más grande del mundo: 300 millones de segmentos a finales de 2009

Como las MT tradicionales, MyMemory almacena segmentos con sus traducciones, ofreciendo a los traductores correspondencias y concordancias. El proyecto se diferencia de las tecnologías tradicionales por sus ambiciosas dimensiones y por su arquitectura centralizada y basada en la colaboración colectiva. Todos pueden consultar MyMemory o hacer aportaciones a través de Internet; la calidad de las aportaciones será cuidadosamente ponderada.
MyMemory gives you quick access to a large number of translations originating from professional translators, LSPs, customers and multilingual web content. It uses a powerful matching algorithm to provide the best translations available for your source text. MyMemory currently contains 644.377.834 professionally translated segments.

Las memorias de traducción son almacenes compuestos de textos originales en una lengua alineados con su traducción en otras. Esta definición de memorias de traducción coincide literalmente con una de las definiciones más aceptadas de corpus lingüístico de tipo paralelo (Baker, 1995). Por esto se puede decir que las memorias de traducción son corpus paralelos.


\section{Reverb}

ReVerb is a program that automatically identifies and extracts binary relationships from English sentences. ReVerb is designed for Web-scale information extraction, where the target relations cannot be specified in advance and speed is important.

ReVerb takes raw text as input, and outputs (argument1, relation phrase, argument2) triples. For example, given the sentence "Bananas are an excellent source of potassium," ReVerb will extract the triple (bananas, be source of, potassium).

\section{Apache Lucene}
\label{sec:lucene}
Lucene es una librer\'ia de information retrieval, de c\'odigo abierto, escrita en Java y distribuida 
bajo la licencia Apache Software License por la Apache Software Foundation. No est\'a pensada para
usuarios finales sino para ser integrada dentro de proyectos inform\'aticos, resolviendo
la parte de bajo nivel y brindando servicios a trav\'es de un API en diferentes lenguajes de programaci\'on.
Su core es un \'indice invertido como el que describimos anteriormente. La implementaci\'on de un sistema
que utiliza Lucene consta de dos pasos separados:
\begin{itemize}
\item La \textbf{creaci\'on} del \'indice, es por lo general un proceso offline en el cual 
se incorporan distintas fuentes de informaci\'on al \'indice 
\item La \textbf{b\'usqueda} de documentos en el \'indice creado en el paso anterior, a partir de una query 
ingresada por el usuario final. Este proceso se incorpora dentro del flujo `online' del sistema.
El resultado de esta b\'usqueda es una lista de documentos rankeados con un cierto puntaje. 
\end{itemize}

Es importante señalar que si bien el proceso de creaci\'on del \'indice suele estar desacoplado del resto 
del sistema, las fuentes de informaci\'on no tiene por que ser `offline' en el sentido de ser documentos
en un disco local. De hecho, Nutch, otro proyecto de c\'odigo abierto de la Apache Software Foundation es 
un motor de b\'usqueda web basado en Lucene que incorpora un crawler para indexar sitios web. Lucene soporta 
cualquier fuente de informaci\'on que pueda convertirse en texto mediante algoritmia.
\newline
Los conceptos fundamentales de Lucene son: \'indice, documento, campo, t\'ermino y query.
\begin{itemize}
\item Un \'indice contiene un conjunto de documentos
\item Un documento es un conjunto de campos
\item Un campo es el nombre de una secuencia de t\'erminos
\item Un t\'ermino es un token (una palabra)
\item Una query es una lista de t\'erminos conectados con distintos operados l\'ogicos
\end{itemize}

\bigskip
[[Dar ejemplos de una query]]
\bigskip

\section{gwtwiki -Java Wikipedia API}\label{sec:gwtwiki}

Java Wikipedia API (Bliki engine)
http://code.google.com/p/gwtwiki/
Esta librer\'ia tiene m\'etodos \'utiles para trabajar con dumps de wikipedia. La usamos para testear los m\'etodos no estructurados de la tesis.


\section{Modelos de Morphia}\label{sec:modelos-morphia}

Java Wikipedia API (Bliki engine)
http://code.google.com/p/gwtwiki/
Esta librer\'ia tiene m\'etodos \'utiles para trabajar con dumps de wikipedia. La usamos para testear los m\'etodos no estructurados de la tesis.
