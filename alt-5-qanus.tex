\chapter{Implementación sobre QANUS}
\label{chap:qanus}
Como parte de nuestra investigación, construimos un sistema de question answering multilingüe y lo evaluamos utilizando como guia dos tareas monolingual de la competencia QA@Clef '07, una para español y otra para portugués. El sistema está basado en un framework académico de código abierto llamado Qanus. Este framework está empaquetado con una sistema de QA simple, QA-sys, que resuelve ejercicios de la competencia TREC '07 para QA solo en inglés. Nosotros adaptamos este sistema para utilizar como librería de procesamiento de lenguajes a Freeling, generando un sistema baseline multilingüe. A este sistema le agregamos features reseñados en \allref{sec:literatura} y realizamos diferente mediciones sobre las instancias en español y portugués sobre los ya mencionados ejercicios de Clef '07.

La estructura de este capitulo es la siguiente: en \allref{sec:ejecicio-de-clef} se describe con detalle el panorama general de tareas de question answering en la competencia Clef '07, haciendo énfasis en las tareas seleccionadas, en los corpus y las preguntas disponibles y en las decisiones tomadas a la hora de elegir estas tareas; en \allref{sec:sistema} se comenta la adaptación del sistema baseline y las modificaciones realizadas a los algoritmos, {\color{red}mientras que los resultados pueden observarse como apartados en las mismas secciones o como una sección aparte al final}.

\section{Ejercicio de CLEF}
\label{sec:ejecicio-de-clef}
Aca
\subsection{Tareas}
\label{subsec:tareas}
Por su parte, para desarrollar y evaluar mecanismos de dominio abierto resolvimos algunos ejercicios de la competencia de QA organizada por CLEF 
en 2007. 
CLEF (de \textit{Cross-Language Evaluation Forum}) es una organización que busca fomentar la investigación en sistemas de information retrieval cross-language. 
En particular, una vez por año CLEF lanza una competencia de Question Answering multilenguaje, con diferentes corpus y diferentes tipos de ejercicios. Estas competencia permiten obtener un patrón estándar de comparación entre distintos desarrollos y una idea general del estado de arte alcanzado en cada área.
Por ejemplo, la competencia ya finalizada del año 2013, QA4MRE@CLEF2013, (Question Answering for Machine Reading Evaluation) se enfoca principalmente en Machine Reading, tarea que incluye un grado de razonamiento elevado para la computadora\footnote{\url{http://celct.fbk.eu/QA4MRE/}}.

Existen distintas conferencias de evaluación de sistemas QA o de subtareas asociadas (por ejemplo TREC - Text Retrieval Conference \footnote{\url{http://trec.nist.gov/}}-, TAC - Text Analysis Conference \footnote{\url{http://www.nist.gov/tac/}}) - y, a su vez, estas distintas competencias ofrecen distinos llamados a competencias. Elegimos resolver una tarea de la competencia Clef '07  por varias razones (Ver \cite{GuidelineClef07} y \cite{OverviewClef07} para un detalle exhaustivo de la conferencia en cuestión). La razón principal fue la pertinencia de la tarea a evaluar al scope de esta tesis. Muchas competencias exigen un grado de complejidad que excede por mucho lo que puede alcanzarse en el tiempo estimado de una tesis de licenciatura. 
Otra razón fue la disponibilidad y el atractivo de la base de conocimiento para estos ejercicios: utilizan snapshots de wikipedia. 
La competencia del '07 ofrece dos tipos de ejercicios:
\begin{itemize}
\item Mono-lingual: donde el idioma de la pregunta y el idioma de la fuente de información son el mismo
\item Cross-lingual: donde el idioma de la pregunta y el idioma de la fuente de información difieren
\end{itemize}
Los ejercicios consideran los siguientes idiomas: inglés, búlgaro, alemán, español, italiano, francés, holandés, rumano y portugués. Por su parte, algunos
ejercicios utilizan corpus de datos privados de la competencia y otros utilizan como fuente las distintas wikipedias. De los ejercicios que utilizan
wikipedia, implementamos un sistema que responde las preguntas en español, mono-idioma, es decir, ejercicios con preguntas formuladas en español que se responden en base a la wikipedia en español. A su vez, implementamos esta misma solución para el inglés, dado que estaban disponibles las preguntas y señalados los links a los snapshots de wikipedia en inglés, pero no fue posible evaluar sus resultados debido a que las respuestas esperadas no estaban disponibles online y no obtuvimos respuesta de los organizadores de la competencia. El uso estructural de la librería freeling permite que la implementación de soluciones para otros idiomas mediante el set-up del corpus en el idioma y una pequeña configuración. 
Los ejercicios elegidos constan de 200 preguntas agrupadas. Los grupos de preguntas refieren a un tema, inferible a partir de la primer pregunta.
Por ejemplo, el primer grupo de preguntas es:
\begin{itemize}
\item ¿En qué colegio estudia Harry Potter?
\item ¿Cuál es el lema del colegio?
\item ¿En qué casas está dividido?
\item ¿Quién es el director del colegio?
\end{itemize}
Es decir, para cada grupo se debe inferir el \dq{tema} en la primer pregunta para arrastrarlo a la hora de responder las siguientes. Más allá de esta particularidad,
las preguntas son preguntas simples. Más adelante haremos un análisis de las mismas con más detalle.

\subsection{Corpus}
El subset de ejercicios de CLEF'07 que elegimos resolver
utiliza como corpus un snapshot de wikipedia en inglés y otro en español,
ambos anteriores al año 2007.
El set de preguntas completo involucra una serie de documentos para los cuales hay que 
registrarse en la asociación: algunas preguntas se responden en base a wikipedia y otras en base a estos documentos.
Identificamos las preguntas que se responden desde wikpedia chequeando los documentos fuente de las respuestas esperadas.

Wikipedia ofrece diferentes formas para obtener una propia imagen de la enciclopedia.
Es posible incorporar la base de datos de wikipedia a una instalación wikimedia
propia, es posible descargar una imagen estática del sitio en archivos html y también se puede
descargar un archivo xml gigante con todos los artículos. 

En la guía para los participantes de la competencia \cite{GuidelineClef07} hay un link para descargar 
wikipedia en inglés preprocesada de Noviembre de 2006 \footnote{\url{http://ilps.science.uva.nl/WikiXML/}} y dos opciones para descargar wikipedia en español: una imagen estática en html de Noviembre de 2006 \footnote{\url{http://static.wikipedia.org/downloads/November_2006/es/}} y un dump xml de Diciembre de 2006 
\footnote{\url{http://download.wikimedia.org/images/archive/eswiki/20061202/pages-articles.xml.bz2}}

La guía aclara que, bajo responsabilidad de los participantes, se puede usar
cualquier otra versión de Wikipedia, siempre y cuando sea anterior a noviembre / diciembre de 2006.
Además, se pide que las respuestas sean tomadas de ``entradas reales" o artículos de wikipedia y
no de otros documentos (por ejemplo: ``image", ``discussion", ``category", etc).

Los links a páginas de wikipedia en español no estaban más disponibles (ambos respondieron 404), mientras que el formato
preprocesado pedido para el inglés resultaba realmente complejo de instalar y parecía una linea muerta. Por estos motivos,
seguimos la sugerencia sobre el uso responsable de otras wikipedias y utilizamos las siguientes:

\medskip

\begin{center}
\begin{tabular}{ | l | l | l |}
    \hline
    Idioma & Fecha & Tamaño\\ \hline
    Inglés \footnote{\url{http://dump-ingles-1}} & 4 de noviembre de 2006 & 7,6G \\ \hline
    Español \footnote{\url{http://dump-español-1}} & 7 de julio de 2006 & 558M \\ \hline
\end{tabular}  
\end{center}

El mantenimiento de un mismo formato para ambos idiomas nos permitió crear un indexador único para cualquier
dump en xml de una wikipedia. Durante el desarrollo y las pruebas, para evitar tiempos de carga innecesarios y también
para probar, utilizamos otras imagenes disponibles de wikipedia:

\medskip

\begin{center}
\begin{tabular}{ | l | l | l |}
    \hline
    Idioma & Fecha & Tamaño\\ \hline
    Español & 26 de enero de 2007 & 902M \\ \hline
    Español & 14 de junio de 2013 & 7,3G \\ \hline
    Inglés simple & 4 de julio de 2006 & 26M \\ \hline
    Inglés simple & 24 de julio de 2013 & 406M \\ \hline
 \end{tabular}
\end{center}

\subsection{Preguntas}

\section{Sistema}
\label{sec:sistema}
Nuestra implementación está basada en Qanus como framework o arquitectura de diseño de QA y en Freeling como proveedor de servicios de procesamiento de lenguaje multilingue. Qanus (Question-Answering @ National University of Singapore) es un framework de question answering basado en information retrieval y también un sistema de QA funcional simple construido sobre este framework. El proyecto se actualizó por última vez en noviembre de 2012 y contiene herramientas actuales de nlp para inglés (el POS-tagger, el NER-tagger y el Question Classifier de Stanford) y también de information retrieval (índice de búsquedas lucene) bajo licencia de código abierto. El paquete cuenta con un framework (qanus), que cumple un rol equivalente a la arquitectura DeepQA en la organización del proyecto de IBM, y un sistema montado sobre este framework -QA-sys-, equivalente a Watson (Ver Figura ~\ref{fig:Quanus}). 

% La motivación del proyecto es proveer a la comunidad científica un framework para ingresar al mundo de QA de una manera más sencilla y rápida, permitiendo construir nuevos sistemas de QA sobre esta arquitectura. 

\begin{figure}
  \centering
    \includegraphics{graficos/Quanus}
  \caption{El framework Quanus y la implementación QA-sys}
  \label{fig:Quanus}
\end{figure}


La arquitectura de Qanus mantiene la estructura de pipeline típica similar a las reseñadas en la sección \allref{chap:estado-de-arte}. 
Consta de los siguientes pasos:

\textbf{Preparación de la fuente de información: } El primer step del pipeline es preprocesar las fuentes de información para un acceso optimizado en pasos posteriores del proceso. La implementación QA-sys incorpora una base de conocimiento en formato XML de AQUAINT\footnote{\ } a un índice de
búsquedas Lucene. En este paso se incorpora todo el conocimiento offline; bases de conocimiento dinámicas como la web se modelan en el pasos posteriores. \newline

\textbf{Análisis de la pregunta: } Este paso, igualmente genérico, permite la incorporación de
distintos componentes para anotar los tokens de la pregunta con datos
útiles para ser consumidos por el paso 3. Otro procesamiento a
realizar en este paso podría ser la generación de queries
entendibles por los distintos motores de almacenamientos de
información del paso 1. En particular, la implementación trae un
pos-tagger, un ner-tagger y un question classiffier, todos de Stanford.
Hablaremos más de estos componentes más adelante. \newline

\textbf{Generación de respuestas: } En este paso se utiliza la información generada en la preparación
de la base de información y en el procesamiento de la pregunta para
generar una respuesta. También puede incorporarse accesos a la web y
validaciones de las respuestas candidatas. La implementación concreta
evalúa cada pasaje de los primeros n documentos retornados por Lucene
para la pregunta original con una serie de componentes ad-hoc de
distancia para adjudicar diferentes grados de confiabilidad a los
distintos pasajes. \newline


Además, se provee de un cuarto paso opcional (el sistema de QA está
completo con los tres pasos anteriores), para la fase de desarrollo y
de evaluación de la performance del sistema:\newline


\textbf{Evaluación: }Este paso está pensado para evaluar las respuestas generadas y
presentarlas de un modo conciso en la fase de desarrollo.
Básicamente, cruza las respuestas obtenidas contra unas respuestas
esperadas escritas a mano y presenta el total de respuestas dadas
correctamente.\newline

En esta sección de la tesis adoptamos el framework Qanus y adaptamos el sistema QA-sys para 1) incorporar Freeling como librería de herramientas de procesamiento del lenguaje permitiendo un funcionamiento multilenguaje, 2) trabajar sobre el corpus de datos y preguntas de los ejercicios de CLEF '07 de español y portugués descriptos en \allref{sec:ejecicio-de-clef} y, finalmente, 3) incorporamos y evaluamos diferentes mejoras tomadas de la reseña a la literatura científica del apartado anterior \allref{sec:literatura}.

\subsection{Notas de Qanus y QA-sys}

El código está escrito en java y mantiene una interfaz común a
todos los pasos: un controller cuyas responsabilidades son cargar los
componentes y un engine que utiliza los componentes para leer el input,
procesarlo y grabar el resultado. La adaptabilidad del framework está
dada en la posibilidad de incorporar componentes respetando la interfaz
especificada para los mismos o bien, en modificar esta misma interfaz.
Presuntamente, el framework es lo suficientemente abierto para permitir
la implementación de sistemas basados en distintas fuentes de
conocimiento (ontologías, archivos, web) y con distintos modos de
funcionamiento mediante poco esfuerzo de customización.

La implementación llamada QA-sys está desarrollada para correr sobre
el tipo de datos de las evaluaciones TREC 2007 (XML AQUAINT). En el
primer paso, incorpora los XML en este formato a un índice Lucene, en
el segundo paso utiliza anota la pregunta con POS tags, NERs y
clasifica el tipo de respuesta con un clasificador entrenado y luego,
en el tercer paso se busca la pregunta sobre el índice lucene y se
retorna una lista con n documentos rankeados. Estos documentos se
subdividen en pasajes. Luego se aplican diferente algoritmos ad-hoc
dependiendo del tipo de respuesta esperada. \ Por ejemplo, si la
respuesta es un nombre de persona, se ejecuta NER sobre los diferentes
pasajes buscando nombres candidatos, si el tipo esperado es una fecha,
se utilizan expresiones regulares escritas a mano, etc. Finalmente, los
pasajes candidatos se evalúan utilizando heurísticas de proximidad
de los candidatos a la pregunta inicial. Para esto se utilizan
diferentes Scorers que rankean los pasajes según diferentes
características (features) y luego se selecciona alguna priorizando
algunas características sobre otras, dependiendo también del tipo
de respuesta esperada. Por último, el evaluador de resultados mide la
exactitud (\textit{accuracy}): total de respuestas correctas sobre
total de preguntas. QA-sys funciona sólo sobre preguntas del tipo
factoid y, a modo de comparación, el mejor sistema según la TREC
2007, el LymbaPA07 obtuvo un grado de exactitud del 0.706 y el décimo
(Quanta) obtuvo 0.206, mientras que QA-sys logra el 0.119. La
implementación es realmente simple y funciona sólo a modo de
ilustración de lo que puede construirse sobre el framework. 


\subsection{Information Base Processing}
Para la construcción de indices lucene con los dumps de wikipedia usamos la librería gwtwiki (Ver Apéndice~\ref{sec:gwtwiki}).
Los artículos se indexan como documentos con los siguiente campos: \emph{id, title, body y all}. En este proceso se descartan artículos mal formados y 
entradas representado imágenes o discusiones, tal como se sugiere en la guía. 
Por mera curiosidad, tomamos tiempos en la contrucción de estos índices locales sobre versiones de wikipedia.
Estos son algunos tiempos de indexación para distintos dumps sobre una [[detalles de la compu]]:
\begin{center}
\begin{tabular}{| l | l | l | l |}
\hline
Idioma & Tamaño & \# Entradas & Tiempo \\ \hline
es & 50M & \# 16 millones & 2.5min \\ \hline
es & 50M & \# 16 millones & 2.5min \\ \hline
es & 50M & \# 16 millones & 2.5min \\ \hline
es & 50M & \# 16 millones & 2.5min \\ \hline
\end{tabular}
\end{center}


El proceso de creación de índices está ilustrado en la figura \ref{fig:LuceneIndexWriterBoth}



\begin{figure}[H]
  \centering
    \includegraphics{graficos/LuceneIndexWriterWiki}
  \caption{Lucene Index Writer para dumps de Wikipedia}
  \label{fig:LuceneIndexWriterWiki}
\end{figure}


\subsection{Question Processing}
Inclusión de freeling como librería y parametrización de idioma.
QC -aprovechando paralelismo con inglés. No hay QC de otros.
Resultados de QC.
\subsubsection{Experimentación de Question Processing}
\subsection{Answer Retrieval}
\subsubsection{Baseline}
Como baseline de desarrollo y pruebas, adaptamos los algoritmos de Qanus. 
El procesamiento realizado por este módulo del paquete puede estructurarse así:
\begin{itemize}
  \item Filtrado de preguntas
  \item Generación de Queries
  \item Information Retrieval
  \item Extracción y rankeo de oraciones
  \item POS tagging de las mejores oraciones
  \item Heurísticas de AR basadas en el QC
\end{itemize}
En esta subsección vamos a describir esta implementación con detalle, mencionando los cambios más importantes que realizamos para adaptarla a la tarea elegida. En las siguientes comentaremos diferentes modificaciones y algoritmia agregada basada en la literatura. 

El proceso de qanus consiste DETALLE DE LA IMPLEMENTACION DE QUANUS.

Qanus solo trabaja sobre preguntas de tipo "FACTOID". Esto descarta "NO FACTOID" preguntas para español y "NO FACTOID" preguntas para portugués. Esta información depende de los experimentos y las comparaciones.

Para la generación de queries, el sistema baseline tiene 4 algoritmos diferentes según QType, es decir, según el resultado de Question Classifier. 
Los casos son los siguientes:
\textbf{Casos especificos para \sq{HUM:ind}} \\
Hay 3 casos especiales para 'HUM:ind', que se chequean contra regexs. En nuestra adaptación ml del sistema baseline, realizamos estos chequeos contra la traducción al inglés de la pregunta original, disponible en los xml de input. Hacer estadísticas con las preguntas. Descomentar y adaptar código. 

$HUM\_IND\_TYPE1\_IC$

$"[wW]ho/WP (is/VBZ|was/VBD)(([ A-Za-z0-9'\\-]+/NN)*([ A-Za-z0-9'\\-\\?]+/NN)+)") || a_QuestionTextPOS.matches("[wW]ho/WP (is/VBZ|was/VBD) the/DT (([ A-Za-z0-9'\\-]+/NN)*([ A-Za-z0-9'\\-\\?]+/NN)+)"$

$HUM\_IND\_TYPE1$

$"[wW]ho/WP (is/VBZ|was/VBD)(([ A-Za-z0-9'\\-]+/NN)+ of/IN(([ A-Za-z0-9'\\-]+/[A-Z]+)*([ A-Za-z0-9'\\-\\?]+/[A-Z]+)+))")|| a_QuestionTextPOS.matches("[wW]ho/WP (is/VBZ|was/VBD) the/DT (([ A-Za-z0-9'\\-]+/NN)+ of/IN(([ A-Za-z0-9'\\-]+/[A-Z]+)*([ A-Za-z0-9'\\-\\?]+/[A-Z]+)+))"$
			
$HUM\_IND\_TYPE2$

$"[wW]ho/WP( )([^w]+/VBD(([ A-Za-z0-9'\\-]+/[A-Z]+)*([ A-Za-z0-9'\\-\\?]+/[A-Z]+)+))"$

\textbf{Default: FormQuery} \\
El sistema original espera como parámetro, además de la pregunta propiamente dicha, un campo "target", aparentemente definido en la competencia TREC para la que está escrito. 
Utilizamos este valor esperado para completar con el "topic" del grupo de preguntas, lo cual no está permitido en las reglas originales de la competencia. 
El método default FormQuery genera una query que contiene, sin repetir, todas las palabras no-stopwords del campo target y todos los sustantivos, verbos y adjetivos de la pregunta completa según el pos tagger de stanford. 

A partir de la query generada, se hace un pedido al índice lucene. Los documentos son divididos en oraciones de manera indiscriminada, es decir: las oraciones no heredan posición de acuerdo al ranking de los documentos a los que pertenecían.

Las oraciones se evaluan mediante diferentes métricas contra la query generada. La fórmula de ponderación es la siguiente: \\
$l_DocScore += (0.9)*l_CoverageScore + (0.05)*l_FreqScore+ (0.05)*l_ProxScore;$

Siendo estas los descriptas en Anexo tal.

Finalmente, sobre las 40 oraciones mejor rankeadas aplica diferente heurísticas segun el QType.
Define heurísticas para los siguientes casos:
\begin{itemize}
  \item ABBR:exp
  \item ABBR:abb
	\item HUM:gr || ENTY:cremat
  \item HUM:
  \item NUM:period
  \item NUM:count
  \item NUM:
  \item ENTY:
  \item Else
\end{itemize}

\textbf{Adaptación inicial contra inglés}

Como las tareas cross-lingual y monolingual se cruzaban, estaban disponibles también en inglés las preguntas para nuestras tareas. 
Como paso de desarrollo, implementamos una solución sobre una wikipedia en inglés con las herramientas de procesamiento originales del baseline: StanfordNER y StanfordPOS. Lamentablemente, la tarea inglés-inglés no fue propuesta, por lo que no disponemos de las goldstandard answers como las que disponemos para español y portugués. De todos modos, realizamos experimentos sobre este sistema, utilizando la wikipedia en inglés de noviembre de 2006. En el apéndice correspondiente pueden encontrarse resultados sobre estos experimentos.


\begin{longtable}{ | p {4cm}| l | p {4cm} |}
    \hline
    Idioma & Fecha & Tamaño\\ \hline
In what school does Harry Potter study? & Transsexual  &  Colegio Hogwarts de Magia y Hechicería \\ \hline
Which is the school motto? & Moon Unit  &  "Draco Dormiens Nunquam Titillandus" \\ \hline
In how many houses is it divided? & Null  &  * Gryffindor    * Slytherin    * Ravenclaw    * Hufflepuff \\ \hline
Who is the school principal? & George Hearst  &  Albus Percival Wulfric Brian Dumbledore \\ \hline
What do swordfishes look like? & Null  &  son grandes peces predadores altamente migratorios, caracterizados por su pico largo y aplanado \\ \hline
Which is the IGFA record for this kind of fish? & Brato  &  536 kg \\ \hline
When was Amintore Fanfani born? & Mae West  &  6 de febrero de 1908 \\ \hline
And where was he born? & Birmingham  &  Provincia de Arezzo, Italia \\ \hline
Who was Le Corbusier? & Null  &  arquitecto
 francés \\ \hline
What was his real name? & Dr Benjamin  &  Charles-Edouard
 Jeanneret-Gris \\ \hline
Who was Flaubert? & Null  &  novelista francés \\ \hline
In what year did he publish "Bouvard et Pécuchet"? & 1942  &  1881 \\ \hline
Enumerate all his works. & Null  &  * Madame Bovary (1856)    * Salambó (1862)    * La educación sentimental (1869)    * La tentación de San Antonio (1874)    * Tres cuentos (1877)    * Bouvard y Pécuchet \\ \hline
What was the Velvet Revolution? & Null  &  el movimiento pacífico por el cual el partido comunista de Checoslovaquia perdió el monopolio del poder y se volvió a la democracia \\ \hline
What was the Carnation Revolution? & Null  &  levantamiento militar del 25 de abril de 1974 que provocó la caída en Portugal de la dictadura salazarista \\ \hline
Who was António de Oliveira Salazar? & Null  &  dictador portugués \\ \hline
Who composed the song "Grândola, Vila Morena"? & McCartney  &  José Afonso \\ \hline
Where was Bender Bending Rodriguez manufactured? & California  &  México \\ \hline
Who is Philip J. Fry? & Null  &  es el protagonista de Futurama \\ \hline
Who was Hermann Emil Fischer? & Null  &  químico alemán, receptor del Premio Nobel de Química \\ \hline
What prize did he receive in 1902? & distance telephone call  &  Premio Nobel de Química \\ \hline
Who was awarded the Nobel Prize in Literature that year? & Lorentz  &  Theodor Mommsen \\ \hline
Who was Bertha von Suttner? & Null  &  pacifista y escritora austríaca \\ \hline
Who was Marco Pantani? & Null  &  ciclista italiano \\ \hline
Which was his nickname? & Milne  &  Il Pirata \\ \hline
Who was Juan Manuel Fangio? & Null  &  pentacampeón del mundo de Fórmula Uno \\ \hline
What is an obsidian? & Null  &  roca del tipo ígnea extrusiva perteneciente al grupo de los silicatos \\ \hline
What is a "macana"? & Null  &  mazas de madera que utilizaban los guerreros precolombinos \\ \hline
What is a black hole? & Null  &  una estrella o un grupo de estrellas con tal
 densidad que absorbe totalmente toda materia o energía que esté en su
 campo de gravedad \\ \hline
What is virtual reality? & Null  &  un sistema o interfaz informático que genera entornos sintéticos en tiempo real \\ \hline
What is a "pasodoble"? & Null  &  una variedad musical dentro de la forma marcha y posteriormente un estilo de baile \\ \hline
What is a tarantella? & Null  &  Baile popular del sur de Italia \\ \hline
What is Noah's Ark? & Null  &  una embarcación construida por Noé durante el Diluvio Universal \\ \hline
What is a tachograph? & Null  &  un dispositivo electrónico que registra diversos sucesos originados en un vehículo durante su conducción \\ \hline
What is an odometer? & Null  &  un dispositivo que indica la distancia recorrida en un viaje por automóvil u otro vehículo \\ \hline
What is a kinnor? & Null  &  una lira hebrea portátil de 5 a 9 cuerdas \\ \hline
What is INTASAT? & Null  &  el primer satélite artificial científico español \\ \hline
What was the Rainbow Warrior? & Null  &  navío de la
 organización ecologista Greenpeace \\ \hline
Who sank it in 1985? & Al MacInnis  &  los servicios secretos franceses \\ \hline
What is a minesweeper used for? & Null  &  la identificación y destrucción de minas marinas \\ \hline
What is INTELSAT? & Null  &  una red de satélites de comunicaciones que cubre el mundo entero \\ \hline
What is a Non-Governmental Organization? & Null  &  una entidad de carácter privado, con fines y objetivos definidos por sus integrantes, creada independientemente de los gobiernos locales, regionales y nacionales, así como también de los organismos internacionales \\ \hline
What was the Gestapo? & Null  &  la policía
 política del régimen nazi \\ \hline
How many members did it have during the Second World War? & 1976  &  45.000 \\ \hline
What is the International Organization for Standardization? & Null  &  organización internacional no gubernamental, compuesta por representantes de los organismos de normalización (ONs) nacionales, que produce normas internacionales industriales y comerciales. \\ \hline
What was The Black Hand? & Null  &  una supuesta organización anarquista secreta y violenta que actuó en Andalucía a finales del siglo XIX \\ \hline
What is the purpose of the World Meteorological Organization? & Null  &  asegurar y facilitar la cooperación entre los servicios meteorológicos nacionales, promover y unificar los instrumentos de medida y los métodos de observación. \\ \hline
What is UNIDO in charge of? & Null  &  de promover y acelerar la industrialización en los países en desarrollo \\ \hline
What does Industrial Light and Magic do? & Null  &  a producir efectos visuales y gráficos generados por ordenador para películas \\ \hline
In what country is Çatalhöyük located? & South Africa  &  Turquía \\ \hline
Into which sea does the Orentes river flow? & Qinghai Province  &  Mar Mediterráneo \\ \hline
In which country is Hadrian's Wall? & Concordia  &  Gran Bretaña \\ \hline
At what place does Homer Simpson work? & Wikicities  &  en la planta de energía nuclear de Springfield \\ \hline
In which country is Minas Gerais located? & {\color{red}Rio de Janeiro}  &  Brasil \\ \hline
Where is ensaymada considered traditional? & Lima  &  Mallorca \\ \hline
In what city was Natalie Hershlag born? & Votkinsk  &  Jerusalén \\ \hline
Who is the mayor of that city? & Joseph B. From  &  Ehud Olmert \\ \hline
Who designed the Zilog Z80 processor? & Gerald Scarfe  &  Federico Faggin \\ \hline
How many bits did this processor have? & 6  &  8 bits \\ \hline
What are the names of Luke Skywalker's parents? & Null  &  Padmé Naberrie, Anakin Skywalker \\ \hline
Which countries joined the European Union on 1st January of 1995? & Null  &  Austria, Finlandia y Suecia \\ \hline
Who is the Queen of Australia? & Paul Ramadier  &  Isabel II \\ \hline
Who was Pope when the Council of Clermont was held? & Opus Dei  &  Urbano II \\ \hline
What actor plays the Chewbacca role? & Peter Pan  &  Peter Mayhew \\ \hline
What's the name of the Greek god of medicine? & Deimos  &  Asclepio \\ \hline
Who is the director of Nosferatu? & Tove Jansson  &  F.W. Murnau \\ \hline
And who played the main character? & James T. Kirk  &  Max Schreck \\ \hline
Who is the general secretary of UGT? & Pedro Calderon de la Barca  &  Cándido Méndez \\ \hline
When was the terrorist attack on the AMIA? & Suicide bombing  &  18 de julio de 1994 \\ \hline
How many people died on the attack? & 1914  &  85 personas \\ \hline
When did Federica Montseny die? & Die  &  14 de enero de 1994 \\ \hline
Which day was the assault to the Davidians Ranch in Waco? & George W. Bush  &  19 de abril de 1993 \\ \hline
In which year did the Real Zaragoza win the European Cup Winners' Cup? & 1942  &  1995 \\ \hline
In what year was the AVE Madrid-Seville inaugurated? & 1492  &  1992 \\ \hline
In which date did El Corte Inglés purchase Galerías Preciados? & MediaWiki:Variantname-sr-el  &  24 de noviembre de 1995 \\ \hline
What day was Rajiv Gandhi killed? & Indira Gandhi  &  21 de mayo de 1.991 \\ \hline
Which four gods can be represented on a canopic jar? & Null  &  Imset, Hapi, Kebehsenuf, Duamutef \\ \hline
Who used to manufacture Windows 95? & NA  &  Microsoft \\ \hline
Which company does Steve Jobs chair? & Hamilton  &  Apple Computer \\ \hline
What was the name of the GRD secret service? & Francisco de Almeida  &  Stasi \\ \hline
Of what organization is David Stern commissioner? & Wikipedia  &  NBA \\ \hline
Which radio station broadcasts Howard Stern's Show? & England (  &  Sirius \\ \hline
Which school was founded by Walter Gropius? & Dr Un Yong Kim  &  Instituto de  Artes y
 Oficios Bauhaus \\ \hline
What order did Saint Thomas Aquinas belong to? & Opus Dei  &  Dominicos \\ \hline
What order was founded by Francis of Assisi? & People  &  Franciscanos \\ \hline
Which military forces did Rommel lead? & Rome  &  Afrika  Korps \\ \hline
What company has Bibendum as mascot? & Guinness  &  Michelin \\ \hline
What was the name of Cousteau's ship? & " Nobody  &  Calypso \\ \hline
What sort of milk is used for making mozzarella? & milk  &  leche de búfala \\ \hline
What is the name of King Arthur's sword? & Elohim  &  Excalibur \\ \hline
Which tree bears acorns? & tree  &  encinas \\ \hline
From what plant are tigernuts obtained? & oils  &  juncia almendrada \\ \hline
What material is a Moai made of? & something  &  roca volcánica \\ \hline
What does the bearded vulture eat? & church  &  médula \\ \hline
What was the name of the atomic bomb dropped on Nagasaki? & name  &  Fat Man \\ \hline
Which was the first book printed by Gutenberg? & Document   &  la Biblia \\ \hline
What form does the entrance of Louvre Museum have? & June  &  pirámide \\ \hline
What animals pull Cybele's chariot? & Cattle  &  leones \\ \hline
What does Mohs' Scale measure? & explanation  &  la dureza de una sustancia \\ \hline
What liquor is made from blackthorn? & liquors  &  pacharán \\ \hline
In what museum is Las Meninas of Velázquez? & Paris  &  Museo del Prado \\ \hline
What scandal caused Nixon's resignation? & August  &  Watergate \\ \hline
What does Leica manufacture? & government  &  instrumentos ópticos de precisión \\ \hline
On what side of a boat is port? & nuisance  &  lado izquierdo mirando hacia proa \\ \hline
What are the names of the balls used in Quidditch? & Null  &  quaffle, bludger y snitch \\ \hline
How many people died in the sinking of the USS Maine? & 60,000  &  266 \\ \hline
How many athletes participated in the Olympic Games of Barcelona? & 6  &  9.364 atletas \\ \hline
How many inhabitants does Lithuania have? & 5  &  3,7 millones \\ \hline
How many seats does the parliament of Iceland have? & 15  &  63 \\ \hline
How many years will the global oil reserves last? & 1731  &  43 años \\ \hline
How many times has Alain Prost been World Champion? & 25  &  cuatro \\ \hline
How much horsepower do the turbines of the USS Nimitz have? & one  &  260.000 HP \\ \hline
How many goals were scored overall at the 1982 Football World Cup? & 500  &  146 \\ \hline
How many basements does the Picasso Tower have? & {\color{red}5}  &  5 \\ \hline
How many Goya Awards did "Torrente: El brazo tonto de la ley" win? & 1934  &  dos \\ \hline
How old is the sun? & 1899  &  4500 millones de años \\ \hline
Which is the running time of the movie "The Ninth Gate"? & Paige  &  132 minutos \\ \hline
At what age did Alfred Hitchcock die? & Two  &  80 años \\ \hline
What is the span of the Airbus 380? & Airbus A380  &  79,75 metros \\ \hline
How many passengers did it transport on 4 September 2006? & 1976  &  474 \\ \hline
How much does each unit cost? & one  &  250 millones de EUR \\ \hline
Which is the carrying capacity of the Antonov An-124? & Tennessee  &  130 toneladas \\ \hline
Who are the founding members of the Star Alliance? & Null  &  Air Canada, Lufthansa, SAS Scandinavian Airlines, Thai Airways International, United Airlines \\ \hline
In what country is náhuatl spoken? & Indonesia  &  México \\ \hline
How many curves does the Circuit de Monaco have? & 5  &  18 \\ \hline
Who was the first woman to go in space? & Garfunkel  &  Valentina Vladimírovna Tereshkova \\ \hline
Who has directed "The Day of the Beast"? & Peter Jackson  &  Álex de la Iglesia \\ \hline
Who inaugurated the Temple of Debod in Madrid? & Joseph Smith  &  Carlos Arias Navarro \\ \hline
In which year was Torquemada appointed Grand Inquisitor? & 1712  &  1482 \\ \hline
In what year was the Nebrija's Grammar published? & 2004  &  1492 \\ \hline
Where did Rachel Weisz study literature? & Freud  &  Trinity Hall, Cambridge \\ \hline
In which city is the church of La Sagrada Familia by Gaudí located? & Poland  &  Barcelona \\ \hline
How high is Mount Vesuvius? & )  &  1.281 metros \\ \hline
How many kilometers away is Loarre Castle from Huesca? & 420  &  35 kilómetros \\ \hline
Which is the molar mass of Methanol? & Carbon  &  32.04 uma \\ \hline
How high is the Pyramid of the Sun of Teotihuacan? & one  &  65 m \\ \hline
How many falls form the Iguazu Waterfalls? & One  &  275 saltos \\ \hline
How many steps does the Tower of Pisa have? & 27  &  294 escalones \\ \hline
Which are the ingredients of sangría? & Null  &  * Vino tinto.    * Fruta picada o rebanada.    * Un endulzador como la miel.    * Un poco de brandy, triple sec, u otro licor. \\ \hline
Who is in charge of the security of the Vatican City? & Pedro Calderon de la Barca  &  Guardia Suiza \\ \hline
Who gives the Fields Medal? & Karl Marx  &  la Unión Matemática Internacional \\ \hline
For whom does Jack Bauer work? & Louise Caroline Alberta  &  la agencia UAT \\ \hline
What was used as defoliant in the Vietnam War? & {\color{red}Herbicides}  &  Agente Naranja \\ \hline
Which was the codename of the seaborne invasion of Normandy? & York  &  Operación Overlord \\ \hline
From which mold is the penicillin obtained? & one  &  Penicillium notatum \\ \hline
From what mollusc did Phoenicians extract purple dye? & rainbow  &  Murex brandaris \\ \hline
What did Manuel Campello Esclápez discover? & use  &  Dama de Elche \\ \hline
What did Jason and the Argonauts look for? & times  &  vellocino de oro \\ \hline
Who was the US president during the attack on Pearl Harbour? & U. Dryfuss  &  Roosevelt \\ \hline
What did Norsk Hydro use to manufacture during the World War II? & Jews  &  agua pesada \\ \hline
Which is the title of the Nadal Prize's winner in 1994? & Rio Reiser  &  Azul \\ \hline
How many reservists did the Haganah have in 1936? & 6  &  40.000 \\ \hline
How old was Miguel Induráin during the 1985 Tour of Spain? & , 1920/CD  &  20 años \\ \hline
In what year did he win the Tour of Spain? & 1185  &  NIL \\ \hline
How many stages did he win in "Tour de l'Avenir" in 1986? & 1942  &  Dos \\ \hline
What prize was he awarded in 1992? & McEwan  &  Premio Príncipe de Asturias de los Deportes \\ \hline
What actors are the main characters of "The Good, the Bad and the Ugly"? & Null  &  Clint Eastwood, Elli Wallach, Lee Van Cleef \\ \hline
How many people died in Europe during the plague epidemic of the 14th Century? & 1810  &  25 millones \\ \hline
In which country did the 1918 flu pandemic start? & Sarajevo  &  Tíbet \\ \hline
What are the Catholic Monarchs' names? & Null  &  Fernando de Aragón e Isabel I de Castilla \\ \hline
What is the name of the president of Burundi who died in 1994? & God  &  Cyprien Ntaryamira \\ \hline
Which day was the Spanish constitution of 1812 promulgated? & War of 1812  &  19 de marzo \\ \hline
Who was the captain of the "Real Sociedad" between 1974 and 1989? & Monty Python  &  Luis Miguel Arconada Echarri \\ \hline
In what city was the final of the 1996 European Basketball Cup held? & Portugal  &  Vitoria \\ \hline
Which terrorist group committed a terrorist attack during the Munich Olympic Games? & Francisco de Almeida  &  Septiembre Negro \\ \hline
What political party ruled Spain from 28 October 1982 until 3 March 1996? & Crimean War  &  Partido Socialista Obrero Español (PSOE) \\ \hline
What uniform did the King wear when intervened on television after Tejero's coup d'état? & coup d'état  &  Capitán General de los Ejércitos \\ \hline
What did Leonardo Da Vinci draw in 1492? & Leonardo da Vinci  &  Hombre de Vitruvio \\ \hline
What did Antonio Rebollo use to light the Olympic Torch in the Barcelona Olympic Games? & article  &  una flecha encendida \\ \hline
What TV show did Takeshi Kitano present from 1986 to 1989? & Opus Dei  &  Humor Amarillo \\ \hline
What gift was New York given at the commemoration of the centennial of the USA independence? & John Wayne Gacy  &  La Estatua de la Libertad \\ \hline
Which is the distributor of the film "Planet of the Apes" which was premiered in 1968? & Sagan  &  20th Century Fox \\ \hline
What speed did Louis Blériot reach on 25 July 1909? & test tube baby  &  64 Kms/hr \\ \hline
How many kilos of anchovies did the Cantabric fleet catch during 1994? & 220  &  14 millones de kilos \\ \hline
What island did Argentina dispute to the United Kingdom between 2 April and 14 June 1982? & U. China  &  Islas Malvinas \\ \hline
Where did the Mediterranean Games of 1997 take place? & Iraq  &  Bari \\ \hline
Which boat did James Cook command from 1768 to 1769? & Kamehameha  &  HM Endeavour \\ \hline
Who plays the main character in the movie "Tarzan the Ape Man"? & Plankton  &  Johnny Weissmüller \\ \hline
How many world records did he break? & 40  &  67 \\ \hline
Who won the Academy Honorary Award in 2007? & Linda Hunt  &  NIL \\ \hline
Who stole Bianca Castafiore's jewels? & Buffy Summers  &  NIL \\ \hline
In what country is The Swan Lake located? & Virginia  &  NIL \\ \hline
In what year did Captain America die? & 1778  &  NIL \\ \hline
Which is the capital city of Neverland? & Catalonia  &  NIL \\ \hline
What is the name of Bill Gates' wife? & Dorion Sagan  &  Melinda French \\ \hline
In what university was he studying when Microsoft was created? & 1167  &  Universidad de Harvard \\ \hline
What budget did that university have in 2005? & times  &  25.900 millones de dólares \\ \hline
In what year was it founded? & 1903  &  1636 \\ \hline
Which supersonic plane finished its commercial flights in May 2003? & Kaho'olawe  &  El Concorde \\ \hline
How many passengers could this model of plane carry? & 1976  &  188 \\ \hline
Which was the capacity of the Santiago Bernabéu Stadium in the 80s? & Seville  &  90.800 espectadores \\ \hline
Who is the owner of the stadium? & God  &  Real Madrid Club de Fútbol \\ \hline
How much money was spent during its enlargement between 2001 and 2006? & 10  &  127 millones de euros \\ \hline
How many bombs were dropped over Dresden on 14 February 1945? & 1972  &  1.800 bombas explosivas y 136.800 bombas incendiarias \\ \hline
How much did the bombs dropped on this city on 7 October 1944 weigh? & one  &  80 toneladas \\ \hline
Which organization did Alan Turing lead during the Second World War? & Kamakura Period  &  la sección Naval Enigma del Bletchley Park \\ \hline
Who was the Russian Tsar during the Second World War? & Meiji  &  NIL \\ \hline
How many points did Yugoslavia score in the 1990 Basketball World Championship final? & 587  &  92 \\ \hline
Against which country did Spain play the final of basketball at the 1984 Summer Olympics? & Netherlands  &  Estados Unidos \\ \hline
In what year did Spain win the Football World Cup? & 1185  &  NIL \\ \hline

\end{longtable}

\textbf{Baseline multilingüe}



\subsubsection{Group Entity extraction}
Formas de extraer la entidad de grupo o tópico y de incorporarlo en la query.
\subsubsection{Query Generation}
Baseline de Qanus.
Resultados.
\subsubsection{Passage Extraction}
\subsubsection{Lasso Heuristics}
Métricas de medición: definición de documento relevante.
\subsubsection{Qanus baseline}
\subsubsection{Heuristic 1}
\subsubsection{Heuristic 2}
\subsubsection{Heuristic 3}
\subsubsection{Experimentación de Answer Retrieval}

% \chapter{Implementación}
% \label{chap:implementacion}

% \section{Frameworks}

% \subsection{No funcionales}

% Para la implementación de nuestro sistema, originalmente, evaluamos la
% utilización de distintos frameworks disponibles. DeepQA, el producto
% de IBM, no es de código abierto, por lo que acerca de su
% implementación sólo sabemos lo que ventilaron en sus artículos
% técnicos. Just.ask, el sistema basado en web comparado contra
% OpenEphyra no está disponible en la web al momento de escribir este
% trabajo, mientras que OpenEphyra no funciona tal cual está dise\~nado
% originalmente (basado en web), sino que el autor sugiere unos pasos
% esotéricos para configurarlo para usar conocimiento local. Cabe
% destacar que esta falla en la funcionalidad está asociada a la que
% había encontrado [AUTOR DE PAPER EPHYRA1] en Aranea y está
% vinculado con una serie de medidas restrictivas tomadas por las
% compa\~nías de buscadores, que fueron cerrando sus accesos gratuitos
% para la comunidad de investigación bloqueando sus APIs y el acceso
% automático a sus UI. Las alternativas para el uso de buscadores,
% actualmente, se reducen a la configuración de una serie de proxies
% sobre los que rotar el acceso a la UI y así enga\~nar al detector de
% accesos automáticos -alternativa de legalidad cuestionable - o bien al 
% pago por una quota de queries por mes.
% OpenEphyra sobrevivió a Aranea porque sus responsables escribieron
% una interfaz para Bing cuando Google cerró sus puertas, mientras que
% los responsables de Aranea no lo hicieron. Finalmente, Bing también
% bloqueo el acceso automático gratuito. Notar que el mismo tipo de
% discontinuación ocurrió con el API de traducciones de Google. La
% empresa declara, explícitamente, que no está dispuesta a acceder a
% ninguna quota de acceso gratuito para la investigación académica y
% que todos sus servicios son pagos. 

% %Tanto de Aranea como de OpenEphyra podríamos llegar a tomar algunos de
% %sus componentes a la hora de construir nuestro sistema. Por el momento,
% %fueron simplemente dejados de lado.

% \bigskip

% \subsection{Qanus}

% Finalmente, un sistema que \textit{sí} estaba disponible y funcionando
% fue Qanus, que respetaba al pie de la letra su detalle técnico. Al comienzo
% del proyecto, contábamos con un corpus de datos en XML,
% lo cual coincidía, al menos en gran parte, con el input esperado de
% la implementación Qa-sys. A pesar de esto, la adaptación de los
% componentes no fue nada trivial y requirió un tiempo excesivo. En
% particular, existían dos opciones a la hora de construir un sistema
% sobre la arquitectura Qanus: dejar de lado la implementación Qa-sys e
% implementar todos los componentes de cero sobre la arquitectura, respetando las interfaces
% dadas por el framework, o adaptar el sistema funcionando para que
% trabaje sobre los nuevos datos y el nuevo entorno esperado. Frente a
% esta alternativa, se aparece claro que el framework en sí mismo no
% aporta demasiado, pues lo único que hace es atar la implementación
% final a una interfaz estructurada de tres procesos bastante sencillo.
% Además, existe un cierto grado de dependencia de la arquitectura
% hacia la implementación final, quizás no a nivel técnico, pero si
% en el modo en el que está definida la estructura. Por este motivo,
% encaramos una adaptación de Qa-sys a nuestro modelo de datos y a
% nuestros requerimientos, pero los resultados no fueron buenos en
% términos de resultados por tiempo invertido. El tiempo de aprendizaje
% del framework mismo y el tiempo requerido para adaptar las distintas
% componentes propias a las interfaces esperadas por Qanus es demasiado
% alto para la solución que brinda. Como recién mencionamos, en
% realidad, el proceso de pipeline de tres pasos no tiene tantas aristas,
% y adaptarse a un framework es mucho menos ameno que escribirlo. Este
% puede ser uno de los motivos por los cuales, como acertadamente
% se\~nalan los autores de Qanus, no existe ningún framework
% estandarizado dentro del ámbito de la investigación en QA.
% Después de la investigación inicial, podríamos concluir que
% está estandarizado, al menos a modo conceptual, la idea de que la
% resolución del problema se debe enfocar como un pipeline de al menos
% tres pasos que incluyen:
% \begin{itemize}
% \item el preprocesamiento de la base de conocimiento,
% \item el preprocesamiento de la pregunta,
% \item el retorno de la respuesta a partir de los resultados de los dos pasos anteriores. 
% \end{itemize}
% Como último comentario al respecto, el modelo de Qanus resultaba poco atractivo
%  a la hora de incorporar procesamiento en varios idiomas: el mejor approach
% utilizando esta arquitectura era implementar dos sistemas basados en
% Qanus paralelos y utilizar uno u otro de acuerdo con el resultado de
% una detección inicial. 

% \bigskip

% Si bien el modelo de Qanus fue, por los motivos recién expuestos,
% dejado de lado, debemos destacar una serie de puntos en los que fue
% útil.

% En primer lugar, uno de los objetivos de Qanus es facilitar el ingreso
% al área del QA de nuevos investigadores. Creemos que esto está
% logrado perfectamente: el código es muy sencillo y claro y lo mismo
% ocurre con la documentación, lo que hace de Qanus un proyecto muy
% útil desde una perspectiva pedagógica o educativa, más allá de
% que sea esta misma simpleza la que más adelante atente contra la
% usabilidad. El modelo de pipeline, que es el enfoque teórico usual al
% problema, y los distintos componentes y usos típicos de estos
% componentes en los distintos pasos del pipeline se realizan linealmente
% en la implementación de Qanus y Qa-sys. En particular, la similaridad
% entre la descripción del código de IBM (DeepQA y Watson) y el
% enfoque con el que Qanus ataca el mismo problema salta a la vista,
% considerando la diferencia de escalas. 

% En segundo lugar, en el plano de la investigación del estado de arte
% de los sistemas de QA disponibles creemos que el intento con Qanus
% redundó en un cierto escepticismo sobre la posibilidad de resolver
% nuestro problema utilizando herramientas disponibles de gran escala. La
% conclusión es análoga a la que tuvo el equipo de IBM al intentar
% usar OpenEphyra y PIQUANT: el tiempo de customización y adaptación
% de los framework a nuestro problema puntual es demasiado alto en
% comparación con el tiempo necesario para construir una nueva
% arquitectura que cumpla los mismos requisitos. 

% Por último, en un nivel técnico, Qanus nos resultó de utilidad para
% construir nuestro modelo final pues reutilizamos varios de los
% componentes de Qa-sys: En primer lugar, recuperamos el POS tagger, el
% NER tagger y el Question Classiffier (QC) de Stanford, que son las
% librerías principales con las que Qa-sys encara el procesamiento
% lingüístico de la pregunta y parte del proceso de generación de
% respuestas. Todas estas herramientas están disponibles en la web por
% otros medios, pero algunas -principalmente el QC- requieren un cierto
% tiempo de configuración inicial que los autores de Qanus ya habían
% resuelto. Es decir, reutilizamos, además de estos módulos externos,
% bastante de la configuración y las APIs de acceso a estos módulos
% escritos por los singapurenses. Estas herramientas funcionan bien
% sólo para inputs en inglés. Las adaptaciones que hicimos las
% veremos más abajo. Por otro lado, incorporamos casi sin
% modificaciones algunas métricas de distancia entre pasajes que Qa-sys
% usa en el momento de la generación de la respuesta como Scorers.
% Estos son las clases: \textbf{FeatureSearchTermCoverage},
% \textbf{FeatureSearchTermFrequency}, \textbf{FeatureSearchTermProximity},
% \textbf{FeatureSearchTermSpan}. Explicaremos esta métricas en breve, dentro de nuestro
% modelo, bajo en nombre de
% {\textquotedblleft}Comparadores{\textquotedblright}. Este código
% está escrito por los autores de Qanus (es decir, no es una librería externa utilizada por ellos).


% \bigskip

% \section{Arquitectura}

% \subsection{Motivación}

% Después del intento con Qanus, decidimos implementar el sistema por
% fuera de cualquier framework y encaramos el dise\~no actual. En este
% dise\~no respetamos el modelo típico de pipeline de tres pasos que
% abunda en la literatura científica y, por lo demás, parece el
% indicado a la hora de encarar este tipo de problemas. Un momento no-técnico
% importante a destacar es la obtención de una base de datos en
% mongodb, resultado del trabajo del proyecto MITIC, la cual cambió
% sustancialmente el enfoque anterior, basado en XMLs. 
% A partir de estos datos, fue posible delinear un esquema de entidades formal que determinó
% qué se puede responder y qué no. Como vimos, la estrategia de QA
% cuando el tipo de datos es estructurado es radicamente distinta que la
% estrategia cuando los datos son no estructurados. Qanus, por su parte,
% está orientado a un tipo de datos no estructurados: buscar documentos
% rankeados en un índice de búsqueda y rastrear en ellos pasajes
% mediante distintos métodos. Cuando la base de conocimientos consta de
% un tipo de datos estructurado (esto es, de entidades, relaciones,
% atributos de entidades) es posible delimitar una ontología más
% rígida que permita concentrarse en la interpretación de la pregunta
% hacia un lenguaje formal. El arquetipo de este enfoque puede pensarse
% como la traducción de un lenguaje de consulta humano a un lenguaje de
% consulta formal, como por ejemplo, SQL: esta estrategia de QA puede
% entenderse \ como una interfaz inteligente a una base de datos. \ En
% eje principal en este acercamiento está en el análisis
% lingüístico de la pregunta a fin de mapearla a un dominio conocido
% y, por otro lado, no es necesario hacer análisis lingüístico
% sobre el corpus de datos.

% Por otro lado, dado que nuestro objetivo inicial incluía desarrollar
% métodos de QA con soporte bilingüe basados en textos (datos no estructurados) 
% y esta base de conocimiento no lo permite, incorporamos al proyecto la resolución
% algunos de los ejercicios de la competencia QA4MRE (Question Answering for Machine Reading Evaluation)
% de la CLEF (Cross-Language Evaluation Forum) del a\~no 2007. 

% Estos ejercicios fueron elegidos para poder evaluar nuestros métodos de análisis lingüsticos,
% ya que la naturaleza del dominio de conocimiento del proyecto mitic - por el formato de sus datos
%  y por lo puntual de su temática- hace imposible cualquier métrica de evaluación objetiva.
% Tras investigar distintas competencias y métodos de evaluación, optamos por la CLEF del 2007
% por contar con un subconjunto de ejercicios bastante adecuados para nuestro sistema mientras que el corpus
% de datos necesario para completar estos ejercicios estaba (en parte) disponible dentro de los tiempos 
% requeridos por nuestro proyecto. 

% \subsection{Base de datos de dominio cerrado}
% \subsection{Ejercicios de dominio abierto}
% \bigskip

% \subsection{Modelo}

% Nuestro sistema resuelve dos problemas de QA similares pero distintos. 

% Por un lado, disponemos de una base de datos estructurada, con datos del área de la investigación y la
% producción en TICs en Argentina, que requiere un enfoque estructurado con métodos basados en ontologías y en 
% formalizaciones de dominio. El enfoque aquí es traducir la pregunta formulada en lenguaje humano a un lenguaje más
% formal ``comprensible" según el modelo de dominio. 

% Por otro lado, para evaluar métodos de QA 
% para datos no estructurados, resolvimos algunos ejercicios de la competencia CLEF del '07. La base de conocimientos para
% estos ejercicios son algunos snapshots de Wikipedia en Espa\~nol y en Inglés, anteriores al 2007. Sobre esta base de conocimiento,
% el enfoque no es ``traducir" la pregunta a un lenguaje estructurado sino interpretarla y ``compararla", mediante distintas métricas, 
% con documentos y pasajes, buscando medidas estádisticas y otras condiciones que permitan \textit{rankear} una respuesta candidata
% con un cierto grado de confianza, o bien determinar que no fue posible encontrar una respuesta para la pregunta. 


% Conceptualmente, el modelo consiste en los tres pasos típicos: la
% creación de la base de conocimientos optimizada, el análisis
% lingüístico de la pregunta y la generación de una respuesta desde
% la base de conocimientos optimizada a partir de la pregunta con sus
% anotaciones. 
% Los pasos 1 y 2, la generación de la base de conocimientos optimizada y el procesamiento de la pregunta son procesos
% esencialmente análogos para la base de conocimientos estructurada y para la no estructurada. El tercer paso, la generación de respuestas,
% tiene distintos enfoques según el caso. 
% En las siguientes secciones recorreremos ambos enfoques en conjunto, señalando las decisiones de dise\~no y los distintos módulos
% utilizados, haciendo las distinciones pertinentes cuando sea necesario.

% En las secciones subsiguiente comentaremos las implementaciones de los
% distintos pasos, con sus decisiones de dise\~no y las tecnologías
% utilizadas. 


% \bigskip

% \section{Base de conocimiento}
% \bigskip


% La creación de la base de conocimiento es el único que se ejecuta offline
% y consiste en la obtención del corpus original y en la generación de índices
% invertidos. Los índices invertidos, recordamos, son índices que permiten
% buscar, para una serie de términos vinculados lógicamente, un conjunto
% ponderado de documentos pertinentes. Por su naturaleza, este paso está 
% separado de la ejecución del resto del código.


% La base de conocimiento de los ejercicios de Clef '07 es mucho más sencilla 
% porque no existe ningún modelo \emph{a priori} más allá del documento de lucene.
% El formato de la entidad ``artículo", como señalamos antes, es: $(id, titulo, cuerpo)$. 
% En este caso, el trabajo más fino no está en el modelado inicial del dominio 
% sino la capacidad lingüística de extraer pasajes a partir de un artículo y recomponer información
% estructurada a partir de estos pasajes. Mientras que para un modelo estructurado la base de conocimiento
% debería permitirnos, para un cierto input, identificar univocamente una entidad y darnos pistas sobre un pedido de
% información acerca de esa entidad, el objetivo sobre un corpus de documentos en 
% traer todos los documentos en los que sea posible que exista un pasaje respondiendo a la pregunta o
% evidencia relevante para apoyar una respuesta. Es decir, mientras una respuesta acotada es una virtud para
% el manejador de una base de conocimientos estructurada, el manejador de una lista de documentos de texto debería devolver
% una lista lo suficientemente grande para contener la respuesta dentro de los pasajes. La razón de esta política es que si
% por ser demasiado estrictos a la hora de retornar documentos llegasemos a descartar un pasaje candidato válido esto
% redundaría en una baja generar de efectividad, mientras que en pasos subsiguiente será trivial descartar toda información irrelevante
% sin tanto costo. Por eso, el acceso a los indices de wikpedia consta simplemente de un generador de queries similar al recién comentado
% accediendo y acumulando resultados (rankeados) a partir de un $LuceneIndexReader$ común (Ver \ref{sec:lucene} \nameref{sec:lucene} para más información).


% \bigskip

% \section{Análisis de la pregunta}
% \label{sec:qprocess}
% En este paso se realizan diferentes análisis lingüísticos de la pregunta.
% El resultado son distintas características asociadas a la pregunta (anotaciones)
% y distintas entidades semánticas reconocidas útiles para el proceso de generación de respuestas. 
% Las herramientas de procesamiento de lenguaje natural que utilizamos en 
% la implementación de este paso del pipeline 
% incluyen: detección de lenguaje, extracción y verificación de entidades nombradas (NER), 
% de verbos, sustantivos, qwords (qué, quién, cómo) (POS), análisis de n-gramas y categorización por tipo de pregunta (QC).

% El proceso de análisis de la pregunta es bastante similar para ambos approachs (estructurado y no estructurado), por lo
% que comentaremos ambos en simultaneo, mencionando diferencias cuando corresponda. 
% Estructuraremos esta parte de la tesis en las siguientes secciones:

% \begin{itemize}
% \item Detección de idioma 
% \item Detección y verificación de entidades nombradas
% \item Análisis gramatical
% \item Clasificación del tipo de pregunta
% \end{itemize}

% Si bien es cierto que el segundo item está basado principalmente en NER-tagging, el tercero en POS-tagging y el cuarto en Question Clasiffication, 
% cada uno de estos pasos utiliza estas herramientas de diferentes maneras. Por ejemplo, para la detección y verificación de entidades del analisis estructurado, además del NER-tagger también utilizamos la base de conocimiento y el análisis gramatical y, para el español, la clasificación del tipo de pregunta se hace apoyandose en las qwords identificadas por el POS-tagger.



% Los ejercicios de Clef '07 no evaluan detección de idiomas. Los archivos de preguntas están separadas por idioma y no se espera que el idioma se infiera a partir de los textos de las preguntas, sino que es un dato dado al sistema de QA.

% \subsection{Entidades nombradas}
% \label{subsec:impl-ner}

% Para la detección de entidades utilizamos la clase simple de detección (NER) y clasificación (NEC) de entidades de Freeling y el NERC de Stanford (ver \allref{sec:freeling} y \allref{sec:stanford-ner}). Las herramientas de Stanford en general superan a las de Freeling -al igual que el detector de idiomas de Cybozu-, pero solo sirven para inglés. La clasificación utilizada por ambos es la más general de las comentadas en \allref{subsec:nerc}: persona, lugar, organización y otros. 

% Veamos algunos ejemplos de funcionamiento de los módulos de detección de entidades. 

% \begin{center}
% \begin{tabular}{| p {8cm} | l | l | l |}
% \hline
% Texto & Freeling & Stanford & Resultado \\ \hline
% ¿Dónde queda la Universidad de Buenos Aires? & es & es & es \\ \hline
% Where is located the University of Buenos Aires? & en & en & en \\ \hline
% Where is located the Univesidad de Buenos Aires? & en & en & en \\ \hline
% Where is located Universidad de Buenos Aires? &  {\color{red}es} & en & en \\ \hline
% Quién es Carolina Fernandez? & es & es & es \\ \hline
% Who is Carolina Fernandez? &  {\color{red}none} & en & en \\ \hline
% Quién es John McCain? & {\color{red}none} & es & es \\ \hline
% Who is John McCain? & en & en & en \\ \hline
% Dónde vive John McCain y por qué vive allí? & es & es & es \\ \hline
% Where does Carolina Fernandez live and why does she lives there? & en & en & en \\ \hline
% \end{tabular}
% \end{center}

% \medskip

% Mientras la detección de entidades para los ejercicios de Clef se detiene en el reconocimiento de entidades nombradas a nivel lingüísitico, para el sistema estructurado el proceso es un poco más complejo. Esto se debe a que en este caso la detección de entidades es esencial. Si en el proceso de anotado de la pregunta no se logra identificar alguna entidad reconocida por el modelo de datos, entonces se está muy lejos de encontrar una respuesta. Por eso, además de utilizar los modulos NER recién mencionado, agregamos otros algoritmos de detección y, también, verificación de entidades. 

% En principio, verificamos la o las entidades nombradas reconocidas contra la base de conocimiento. El $KnowledgeManager$ ofrece diferentes servicios de verificación de entidades. Para una cadena de tokens cualquiera, este módulo puede decidir, con un cierto grado de confianza, si:

% \begin{itemize}
%   \item La cadena de tokens es una entidad dentro del modelo de datos. Esto incluye:
%     \begin{itemize}
%       \item Es una entidad del modelo de datos: una universidad, una empresa, un investigador, un proyecto, una publicacion o una tematica
%       \item Es una entidad inferida: una ciudad, una provincia, un centro de investigación, un lugar de trabajo
%     \end{itemize}
%   \item La cadena es una colección del modelo de datos, es decir, si se están nombrando \dblquote{Investigadores} o \dblquote{Universidades} como clase de entidades.
%   \item La cadena es un atributo o una relación de una clase. (nombre de investigador)
% \end{itemize}

% Parte importante del trabajo para este esquema es lograr identificar este tipo de entidades lingüísticas, por lo que además de verificar los resultados del proceso de NER-tagging, también generamos otras cadenas de input. Notar además que los nombres de clase y de atributos de clase no tendrían por que ser reconocidas por el NER-tagger. Por ejemplo, para ``¿Qué investigadores trabajan en Córdoba?", \dblquote{investigadores} está haciendo referencia al nombre de una clase pero no es el tipo de entidades lingüísticas que detecta un NER-tagger. 

% Por estas razones, generamos más entidades lingüísticas posibles además de las entidades detectadas por los NER-taggers. Una vez que todas las entidades nombradas fueron verificadas, generamos n-gramas sobre el resto de la pregunta para chequear por más tokens reconocibles. Configuramos la generación de n-gramas de 1 a 3, con ciertos filtros para no verificar construcciones que no representan entidades de manera trivial. Por ejemplo: dejamos sólo los unigramas que cumplan el rol de sustantivos, eliminamos bigramas que sean un sustantivo y una acción, salteamos NERs ya reconocidas, etre otros.


% \subsection{Análisis gramatical}

% De las diferentes etiquetas que generan los POS-taggers, en nuestro sistema distinguimos los verbos, los sustantivos, las qwords y las palabras triviales. 
% Las palabras etiquetadas cumplen distintos roles a lo largo del proceso de generación de respuestas. Como señalamos recién, los n-gramas que se verifican contra la base de datos están filtrados por los roles gramaticales de sus tokens. Por otro lado, a la hora de generar el tipo de pregunta para una pregunta en español, utilizamos, como mecanismo ad-hoc, las qwords. 

% \begin{center}
% \begin{tabular}{| l | l |}
% \hline
% Clase & Ejemplos\\ \hline
% qword  & qué, quién, cómo, dónde, cuándo\\ \hline
% verbo & trabaja, trabajar, trabajando \\ \hline
% trivial  & lo, a, de, y \\ \hline
% sustantivos  & universidad, impresora, álgebra \\ \hline
% \end{tabular}
% \end{center}

% Los usos más intensivos de estas etiquetas son el filtrado de n-gramas que describimos en la sección anterior para el caso estructurado (\allref{subsec:impl-ner}), un algoritmo ad-hoc de etiquetado de Q-Type para el caso español (es el próximo tema a discutir en \allref{subsec:qtype}) y, para la generación de respuestas, las ponderaciones de pasajes en los scorers de los ejercicios de Clef (\allref{subsec:scorers}), y finalmente, sirven para desempatar por atributos o relaciones preguntadas en algunos casos del modelo estructurado.

% \subsection{Clasificación}
% \label{subsec:qtype}
% Para clasificar la pregunta según su tipo de respuesta esperada utilizamos el Question Classiffier de Stanford, tomando la configuración de Qanus. Este clasificador arroja una clase y una subclase (ver \allref{sec:stanford-qc} para una lista detallada de las posibles clases) y un grado de confianza para esta asignación. A continuación presentamos algunos ejemplos que ilustran estos resultados.

% \begin{center}
% \begin{tabular}{| l | l | l |}
% \hline
% Pregunta & Clase y Subclase & Confianza\\ \hline 
% What's his name? & HUM:ind & 0.74 \\ \hline 
% Where do you come from? & DESC:desc & 0.62 \\ \hline 
% What's your phone number? & NUM:code & 0.63 \\ \hline 
% How old are you? & NUM:period & 0.78 \\ \hline 
% When were you born? & NUM:date & 0.99 \\ \hline 
% What does he look like? & DESC:desc & 0.82 \\ \hline 
% \end{tabular}
% \end{center}

% Sin embargo, como ya señalamos oportunamente (ver \allref{subsec:qc}), no existen herramientas de clasificación de preguntas para el idioma español. Esto nos llevó a tomar diferentes medidas para aproximar un tipo de pregunta y no tener distintos casos de código para el inglés y el español. Para las preguntas de Clef formuladas en español, utilizamos su versión en inglés para obtener el tipo.
% Así, el tipo de respuesta esperada de \dblquote{¿En qué colegio estudia Harry Potter?} es el mismo que el tipo de respuesta esperada de ``In what school does Harry Potter study?" (ENTY:cremat con 0.22 de confianza). Además, utilizamos reglas escritas a mano sobre QWords. Las qwords son palabras clave de las preguntas que señalan el tipo de respuesta. Por ejemplo: una pregunta que comienza con `Cuándo' tendrá como tipo de respuesta una fecha, un tiempo, etc. En el modelo estructurado, definimos una serie acotada de categorías de tipo de respuesta esperadas y unificamos los resultados del clasificador de Stanford y nuestras reglas sobre qwords para español para unificar el código. Estas categorías son:  Who, Whom, Where, Which,  When,  What y Other. Es decir: Quién, Quiénes, Dónde, Cuál, Cuándo, Qué y otros. Las clases y subclases del clasificador de Stanford se mapearon a estas categorías que coinciden con los resultados de las reglas escritas a mano para el español. 


% \subsection{Entidad de Grupo}
% \label{subsec:entidad-de-grupo}

% El formato de las preguntas para los ejercicios Clef que elegimos es un xml para cada idioma. En particular, nosotros resolvimos los ejercicios en español y inglés que podían responderse en base a wikipedia. Contar del tema de los grupos. Poner algún ejemplo.

% \subsection{Analisis de las preguntas de Clef}

% De las respuestas goldstandard, discriminamos las soportadas por wikipedia, las soportadas por el corpora para miembros registrados y las que no tienen respuesta (NIL):

% \begin{center}
% \begin{tabular}{| l | l | l | l | l |}
% \hline
% Idioma & Wiki & News & NIL & Total \\ \hline
% es & 155 & 37 & 8 & 200 \\ \hline
% pt & 130 & 59 & 11 & 200 \\ \hline
% \end{tabular}
% \end{center}


% Los resultados positivos esperamos que sean en wiki (news seguro no y nil no sé). 


% Por QTYPe
% \begin{center}
% \begin{tabular}{| l | l | l | l | l | l |}
% \hline
% Subset & Idioma & Factoid & Definition & List & Total \\ \hline
% \multirow{2}{*}{Todo} & es & 158 & 32 & 10 & 200 \\ \cline{2-6}
%  & pt & 159 & 31 & 10 & 200 \\ \hline
%  \multirow{2}{*}{Wiki} & es & 122 & 24 & 9 & 155 \\ \cline{2-6}
%  & pt & 104 & 18 & 8 & 130 \\ \hline
% \end{tabular}
% \end{center}

% Analisis por tipo de respuesta esperada:
% \begin{center}
% \begin{tabular}{| l | l | l | l | l | l |l |l|l|}
% \hline
% Tipo & pt & es & pt-factoid & es-factoid & pt-def & es-def & pt-list & es-list \\ \hline
% COUNT & 21 & 22 & 21 & 22 & 0 & 0 & 0  & 0\\ \hline
% OBJECT & 11 & 27 & 6  & 18 & 5 & 9 & 0  & 0\\ \hline
% MEASURE & 15 & 20 & 15 & 20 & 0 & 0 & 0  & 0\\ \hline
% PERSON & 33  & 35 & 21 & 24 & 9 & 8 & 3 & 3\\ \hline
% TIME & 19 & 16 & 18 & 16 & 0 & 0 & 1 & 0\\ \hline
% LOCATION & 33 & 18  & 30 & 18 & 0 & 0 & 3 & 0 \\ \hline
% ORGANIZATION & 31 & 32 & 24 & 22 & 6 & 8 & 1 & 2\\ \hline
% OTHER & 37 & 30 & 24 & 18 & 11 & 7 & 2 & 5 \\ \hline
% Total & 200 & 200 & 159 & 158 & 31 & 32  & 10 & 10\\ \hline
% \end{tabular}
% \end{center}


% \section{Generación de Respuestas}

% El proceso de generación de respuestas difiere sustancialmente entre ambos modelos de dominio. Para el sistema basado en datos estructurados, el approach es determinan en casos de código las distintas posibilidades, acotadas, de las cosas que se pueden responder. Nuestro dominio es tal que solo se pueden responder entidades, atributos de entidades o relaciones entre entidades. De ese modo, si no es posible redirigir el flujo de la pregunta hacia alguna respuesta conocida, no hay posibilidad de articular una respuesta significativa. Para el caso de los ejericicios de Clef sobre wikipedia, el enfoque es muy distinto. En primer lugar, no hay un modelo de los datos del dominio, hay textos con pasajes (u oraciones). Si bien es posible una cierta jerarquización de los datos (por ejemplo, utilizando los nombres de los articulos como verificación de la existencia de una entidad), un enfoque estructurado resulta imposible. En este contexto se utiliza la técnica de rankeo semántico de pasajes en base a features (características). Estas dimensiones de valoración de los pasajes son llamados Scorers (ver \allref{subsec:scorers}). Los Scorers, como veremos, pueden ser tan sencillos como preferir minimamente una cierta longitud sobre otra y también pueden incorporar dimensiones de análisis lingüístico (por ejemplo, la presencia de cierta entidad en un cierto rol semántico). El algoritmo de generación de respuestas consiste, en el caso no estructurado, en encontrar features útiles, significativos y en establecer mecanismo inteligentes de priorización de estos features. 

% \subsection{Estructurado}

% Una vez etiquetados todos los tokens de la pregunta, se procede a marcar como procesadas las ``palabras triviales". Estas son palabras que si no formaron parte de alguna otra construcción, entonces no haberlas procesado no debería considerarse un problema. Ejemplos de ellas son las proposiciones, los pronombres y algunos conectores. Si al etiquetar la pregunta no se logró identificar ninguna entidad del modelo, entonces será dificil avanzar. Como vimos recién, la fase de procesamiento de la pregunta para el caso estructurado excede por mucho la mera anotación lingüística: al finalizar el etiquetado deberíamos disponer de alguna entidad reconocida por el modelo. Por otro lado, durante la fase de etiquetado cada palabra se marca como procesada. Como acabamos de señalar, al finalizar este proceso se marcan también como procesados ciertos tokens triviales. En este punto, el sistema debe tomar una decisión. Si no ha logrado etiquetar una cierta cantidad de tokens (más del 80\%), entonces se considera que no tiene sentido dar por ``comprendida" la pregunta y se procede a un segundo análisis, computacionalmente más costoso y además más inexacto, en el que se intenta encontrar alguna entidad con otros métodos que enunciaremos en breve. En caso de encontrarse una entidad, entonces el flujo del programa retorna al curso de análisis estructurado. Si esto no ocurre, se devuelve la lista de entidades (documentos) que el índice lucene general devuelve para la pregunta original interpretada como una query normal de information retrieval. Este caso, si bien retorna información, es un caso de falla de procesamiento. Esta lista de documentos viene acompañada de un mensaje del tipo: \dblquote{No se logró interpretar su pregunta} y, en un trabajo futuro, podría incorporar un sistema de recomendaciones e idas y vueltas con el usuario (¿Quizo decir...?).

% Si, en cambio, el threshold de tokens es alcanzado, entonces se pasa a otro switch. En este caso el código se bifurca de acuerdo a la cantidad de entidades del modelo reconocidas. Por entidades del modelo entenderemos, aquí, entidades internas, objetos, no nombres de clase o de atributos. Distinguimos estos tres casos: `ninguna entidad reconocida', `una entidad reconocida', `más de una entidad reconocida'.


% \begin{figure}[H]
%   \centering
%     \includegraphics[scale=0.5]{graficos/AnswerRetrievalFlowEstructurado}
%   \caption{Flow para la Generación de Respuestas - Estructurado}
%   \label{fig:AnswerRetrievalFlowEstructurado}
% \end{figure}

% \subsubsection*{Ninguna o más de una entidad}
% En el caso en el que no se haya identificado ninguna entidad del modelo quedan diferentes posibilidades, que se verifican en orden secuencial en base a nombres de colección, nombres de atributos, tipo de respuesta esperada y verbos. Los casos contemplados son: la pregunta por un campo de una colección (por ejemplo: direcciones de empresas en buenos aires) o por listas de entidades de una colección (investigadores de capital federal). Los diferentes casos son reglas de código escritas a mano. En todos los casos, si no se dan las condiciones para seguir especificando la dirección de la respuesta, se genera un respuesta ad-hoc con datos rankeados según el índice invertido, especificando de modo estructurado el camino recorrido hasta el momento. Por ejemplo, si se identifica que se pregunta por `investigadores' pero no es posible decidir ninguna especificación más (de capital federal, que hayan publicado en 2008, etc) entonces se retorna una lista de investigadores rankeada según el índice invertido `investigadores' con el resto de los datos de la pregunta. 

% Por otro lado, si hay más de una entidad reconocida entonces hay sólo algunos casos posibles de relaciones entre ellas que pueden ser respuestas, que también se reflejan como caso de código. Finalmente, si no es posible identificar ninguno de estos caso, se toma un camino similar al mecanismo ad-hoc basado en information retrieval.


% \subsubsection*{Una entidad}
% El mejor caso es aquél en el que se reconocío una entidad y otros datos lingüisticos que permitan especificar qué se está preguntando. Al especificar acerca de qué/quién resulta mucho más sencillo canalizar qué se está preguntando. Los modelos que representan objetos (ver \allref{subsec:modelos-db}) son subclases de $NodoBase$, el cual representa una entidad en abstracto. Una entidad sabe responder preguntas acerca de ella. Para esto, utiliza las anotaciones de verbos, atributos nombrados y qwords para identificar qué se está preguntando. La pregunta puede responderse con un atributo o con una relación. Los distintos atributos de las distintas entidades se corresponden con verbos y con tipos de respuesta esperada. 

% [[Detalle de casos y combinaciones de verbos + tipo de respuesta esperada + atributo]]
% [[Ejemplos]]

% \subsection{No Estructurado}

% El proceso de generación de respuestas para los ejercicios de la Clef es muy distinto del anterior y puede dividirse en tres pasos principales: obtención de documentos y pasajes, ranking de pasajes y generación de respuesta. En el primer paso se accede a los índices invertido (al corpus) buscando documentos relevantes. Este paso pertenece netamente al área information retrieval. Como mencionamos al comentar Watson (en particular, ver \allref{subsec:deep-qa}), es fundamental que el resultado de este paso sea lo suficientemente amplio como para contener la respuesta pero lo suficientemente acotado como para no sobrecargar el proceso posterior de análisis lingüístico sobre los pasajes. Los documentos rankeados se dividen en pasajes. En el segundo paso, tanto los documentos como los pasajes son contrastados con distintas métricas contra los datos de la pregunta generando distintos valores para estos features. Finalmente, con esta información se procede al tercer paso, que consiste en realizar diferentes filtrados sobre los pasajes en función del tipo de respuesta esperado y en distintas formas de recopilar evidencia a favor de un pasaje o una entidad (depende el caso) para finalmente seleccionar una respuesta (o decidir que no se encontró ninguna).

% \begin{figure}[H]
%   \centering
%     \includegraphics[scale=0.75]{graficos/AnswerRetrievalFlowWiki}
%   \caption{Flow para la Generación de Respuestas - No Estructurado}
%   \label{fig:AnswerRetrievalFlowWiki}
% \end{figure}

% \subsubsection{Documentos}
% \label{subsec:docs}
% En este punto, para una pregunta dada se dispone de la entidad del grupo de preguntas y de las distintas anotaciones hechas a la pregunta en el paso anterior (\allref{sec:qprocess}). Por su parte, los documentos en los índices invertidos poseen los campos $Id$, $Title$, y $Text$. El mecanismo de generación de queries tiene como objetivo priorizar en el ranking los documentos relacionados con el tema asociado al grupo de preguntas. Este paso es un problema de information retrieval puro: esto es, dado un pedido de información, retornar \textit{documentos relevantes}. El análisis semántico tiene peso en el paso posterior, a la hora de rankear pasajes. Por ejemplo, para el primer grupo de preguntas acerca de Harry Potter, solo se espera de este una lista de de documentos relacionados con ese mundo, en primer lugar. Por otro lado, es necesario que los principios de generación de queries no sean demasiado estrictos. Si en este punto quedan afuera muchas ocurrencias de una respuesta, entonces todo el resto del programa se ve afectado de manera irreparable. Es preferible generar documentos de más y luego filtrarlos mediante análisis lingüístico que ser demasiado estrictos y perder respuestas. 
% Para lograr esto, ponderamos los documentos en los que las entidades nombradas reconocidas lingüísticamente aparecen en el título, si se dispone de más de una entidad buscamos documentos que mencionen ambos, luego priorizamos los documentos que poseen estas entidades dentro del cuerpo y también consideramos la presencia de verbos en diferentes conjugaciones y de sustantivos que ocurren en la pregunta. Finalmente, agregamos una lista de documentos enviando la pregunta misma como una query.  

% Dado que finalmente se realizan queries simples (masivas), cabe preguntarse cual es la razón de la generación de queries y la ponderación de documentos. Esta razón es que en el proceso de ranking de pasajes y evaluación de respuesta se utilizan features basados en el score dado por lucene a los diferentes documentos. Si una mejor posición del documento contenedor del pasaje no implica que el pasaje sea correcto, si en cambio es un indicador de que dicho pasaje se encontró más cerca o más lejos del nucleo temático en el que se esperaba encontrarlo. 

% Una vez generada la lista de documentos rankeados según lucene, se procede a analizar algunos features en base a distintos scorers propios. A su vez, estos distintos valores se combinan en una evaluación general del documento, que será utilizada luego a la hora de generar una respuesta. Estas dimensiones buscan en el título y en el artículo diferente medidas sobre las entidades nombradas y sobre la pregunta completa. En concreto, se miden distancias a la entidad nombrada que identifica al grupo de preguntas (ver \allref{subsec:entidad-de-grupo}), las entidades nombradad en la pregunta misma, a la pregunta completa y la respuesta esperada data. Las medidas contra la respuesta esperada -dada por Clef- no pueden usarse para generar la respuesta, pero sí para evaluar la performance del sistema. En el siguiente cuadro se muestran las dimensiones que se consideran sobre los documentos. 

% \begin{center}
% \begin{tabular}{| l | l | l |}
% \hline
% Entidad & Campo & Comparador \\ \hline
% \multirow{6}{*}{Entidad de Grupo} & \multirow{3}{*}{Título} & Span \\ 
% & & Covr \\
% & & Freq \\ \cline{2-3}
% & \multirow{3}{*}{Texto} & Span \\ 
% & & Covr \\
% & & Freq \\ \hline
% \multirow{6}{*}{Entidades de Pregunta} & \multirow{3}{*}{Título} & Span \\ 
% & & Covr \\
% & & Freq \\ \cline{2-3}
% & \multirow{3}{*}{Texto} & Span \\ 
% & & Covr \\
% & & Freq \\ \hline
% \multirow{6}{*}{Todas las entidades} & \multirow{3}{*}{Título} & Span \\ 
% & & Covr \\
% & & Freq \\ \cline{2-3}
% & \multirow{3}{*}{Texto} & Span \\ 
% & & Covr \\
% & & Freq \\ \hline
% \multirow{6}{*}{Pregunta} & \multirow{3}{*}{Título} & Span \\ 
% & & Covr \\
% & & Freq \\ \cline{2-3}
% & \multirow{3}{*}{Texto} & Span \\ 
% & & Covr \\
% & & Freq \\ \hline
% \multirow{6}{*}{Respuesta} & \multirow{3}{*}{Título} & Span \\ 
% & & Covr \\
% & & Freq \\ \cline{2-3}
% & \multirow{3}{*}{Texto} & Span \\ 
% & & Covr \\
% & & Freq \\ \hline
% Score según índice & -- & -- \\ \hline
% \end{tabular}
% \end{center}

% Los comparadores señalados ($Freq$, $Covr$, y $Span$) se utilizan en distintos lugares de esta tesis y su funcionamiento es explicado en el apéndice \allref{sec:comparadores}. Notar que `Entidades de la pregunta' refiere tanto a aquellas reconocidas por el NER-tagger como a construcciones sustantivadas y que `Pregunta' no es la pregunta bruta sino la priorización de verbos conjugados, sustantivos, adjetivos y entidades. 

% El score general del documento es un cálculo ponderado de estas diferentes dimensiones. 

% Lucene permite especificar cuántos documentos queremos recuperar. Para evaluar la performance de este paso, utilizamos medida distintos scores en base a la respuesta dada por dada por la conferencia para el ejercicio. Es importante notar que dado que no utilizamos las imagenes de wikipedia de la primera sugerencia, es esperable que las respuesta no estén `tal cual'. 
% Evaluamos distintos mecanismos de generación de documentos, con distinta cantidad total, bajo distintas métricas. Para generar documentos, probamos la query trivial $ALL: pregunta$ (1), una un poco mejorada $ALL: entidad_de_grupo pregunta$ (2), secuencias concatenadas de queries tal como las describimos más arriba (3) y varios pedidos separados aplanados en un paso posterior (4). Para los cuatro métodos eliminamos los signos de puntuación. Para medir los resultados, utilizamos los comparadores de presencia exacta y diferentes grados de cobertura de términos (.8, .9 y 1). A su vez, evaluamos distintas imagenes de wikipedia para el español. Es total de preguntas del ejercicio, recordamos, es 200. Los resultados son los siguientes.

% \begin{center}
% \begin{tabular}{|l|l|l|l|l|l|l|}
% \hline
% Método & \# Docs & Wikipedia & Exacto & Covr 1 & Covr .9 & Covr . 8 \\ \hline

% \multirow{6}{*}{1 - Trivial} & 
% \multirow{3}{*}{100} & es - 2006 & 132 & 151 & 152 & 159 \\ 
%  &  & es - 2007 & 144 & 159 & 160 & 164 \\
%  &  & en - 2006 & x & x & x & x \\ \cline{2-7}
%  & \multirow{3}{*}{1000} & es - 2006 & 144 & 167 & 167 & 173 \\  
%  &  & es - 2007 & 159 & 177 & 177 & 180 \\
%  &  & en - 2006 & x & x & x & x \\ \hline

% \multirow{6}{*}{2 - Trivial'} & 
% \multirow{3}{*}{100} & es - 2006 & 138 & 156 & 157 & 164 \\ 
%  &  & es - 2007 & 151 & 167 & 168 & 171 \\
%  &  & en - 2006 & x & x & x & x \\ \cline{2-7}
%  & \multirow{3}{*}{1000} & es - 2006 & 147 & 168 & 168 & 174 \\ 
%  &  & es - 2007 & 163 & 178 & 178 & 181 \\
%  &  & en - 2006 & x & x & x & x \\ \hline
% hola & ey &wiki& 137 & 156 & 157 & 160  \\ \hline
% \multirow{6}{*}{3 - Inteligente} & 
% \multirow{3}{*}{100} & es - 2006 & 127 & 144 & 144 & 150 \\ 
%  &  & es - 2007 & 141 & 150 & 151 & 156 \\
%  &  & en - 2006 & x & x & x & x \\ \cline{2-7}

%  & \multirow{3}{*}{1000} & es - 2006 & 143 & 163 & 163 & 170 \\   
%  &  & es - 2007 & 158 & 175 & 175 & 179 \\
%  &  & en - 2006 & x & x & x & x \\ \hline

% \multirow{6}{*}{4 - Inteligente'} & 
% \multirow{3}{*}{100} & es - 2006 & 142 & 160 & 161 & 168 \\ 
%  &  & es - 2007 & 157 & 170 & 171 & 174  \\
%  &  & en - 2006 & x & x & x & x \\ \cline{2-7}


%  & \multirow{3}{*}{1000} & es - 2006 & 147 & 168 & 168 & 174 \\ 
%  &  & es - 2007 & x & 158 & x & x \\
%  &  & en - 2006 & x & x & x & x \\ \hline
 
% \end{tabular}
% \end{center}

% Conclusión de esto.

% \subsubsection{Pasajes}
% Este paso es análogo al anterior, pero con mayor detalle y granularidad. Cada documento generado en el paso anterior, con sus diferentes puntajes para 
% las dimensiones señaladas, se parten en pasajes u oraciones. Nuevamente, sobre estas oraciones realizamos diferentes mediciones y las combinamos generando
% un score final. En esta sección discutiremos las mediciones consideradas y los diferentes métodos de combinación de las mismas. Estos métodos de combinación generan distintos rankings de pasajes. Para evaluar estos rankings, nuevamente, utilizaremos la información disponible sobre las respuestas esperadas, buscando que la respuesta esperada se encuentre entre los $n$ pasajes mejor rankeados.
% En primer lugar, las distintos scorers implementados son los siguientes:

% \begin{center}
% \begin{tabular}{| l | l | l |}
% \hline
% Comparador & Qué & Dónde \\ \hline
% \multicolumn{3}{|c|}{Estadísticos} \\ \hline
% Freq & Pregunta & Pasaje \\ \hline
% Span & Pregunta & Pasaje \\ \hline
% Covr & Pregunta & Pasaje \\ \hline
% \#Tokens & -- & Pasaje \\ \hline
% \multicolumn{3}{|c|}{Basados en NLP} \\ \hline
% Presencia & Entidad de Grupo & Pasaje \\ \hline
% Presencia & Entidades de pregunta & Pasaje \\ \hline
% Presencia & Verbos de pregunta & Pasaje \\ \hline
% Presencia & Sustantivos de pregunta & Pasaje \\ \hline
% \multicolumn{3}{|c|}{Para evaluación} \\ \hline
% Freq & Respuesta & Pasaje \\ \hline
% Span & Respuesta & Pasaje \\ \hline
% Covr & Respuesta & Pasaje \\ \hline
% \end{tabular}
% \end{center}

% A estos Scorers se le suman los scores del documento asociado al pasaje (ver \allref{subsec:docs}). 
% Sobre estas dimensiones disponibles, intentamos las siguientes combinaciones de priorización:

% \begin{center}
% \begin{tabular}{|l|l|l|}
% \hline
% \#& Nombre & Fórmula \\ \hline
% 1& Simple & $2+2=4$ \\ \hline
% 2& Respuesta & $2+2=4$ \\ \hline
% 3& Compleja & $2+2=4$ \\ \hline
% \end{tabular}
% \end{center}

% Y consideramos la ocurrencia de respuestas, de la misma manera que en el apartado anterior (Match Exacto y tres medidas de covertura de tokens: 1, .9 y .8), 
% sobre los primeros $n$ pasajes, con $n$ = 1, 5, 10, 20, 50 y 100.

% \begin{center}
% \begin{tabular}{|l|l|l|l|l|l|}
% \hline
% Fórmula & \#Docs & Exacto & Covr 1 & Covr .9 & Covr . 8 \\ \hline
% \multirow{6}{*}{1} & 1 & x & x & x & x \\  \cline{2-6}
%  & 5 & x & x & x & x \\ \cline{2-6}
%  & 10 & x & x & x & x \\ \cline{2-6}
%  & 20 & x & x & x & x \\ \cline{2-6}
%  & 50 & x & x & x & x \\ \cline{2-6}
%  & 100 & x & x & x & x \\ \hline
% \end{tabular}
% \end{center}


% \subsubsection{Respuestas}





% \begin{figure}
%   \centering
%     \includegraphics[scale=0.86]{graficos/Architecture}
%   \caption{Arquitectura}
%   \label{fig:Architecture}
% \end{figure}

% %\end{document}
